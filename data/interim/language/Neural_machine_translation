cleanup reorganize|date=September 2015

Neural machine translation (NMT) is an approach to machine translation that uses a large artificial neural network to predict the likelihood of a sequence of words, typically modeling entire sentences in a single integrated model.

Deep neural machine translation is an extension of neural machine translation. Both use a large neural network with the difference that deep neural machine translation processes multiple neural network layers instead of just one.<ref name="layers"/>

Properties
They require only a fraction of the memory needed by traditional statistical machine translation (SMT) models. Furthermore, unlike conventional translation systems, all parts of the neural translation model are trained jointly (end-to-end) to maximize the translation performance.<ref name="KalchbrennerBlunsom"/><ref name="sequence"/><ref name="Properties"/>

History
Deep learning applications appeared first in speech recognition in the 1990s. The first scientific paper on using neural networks in machine translation appeared in 2014, followed by a lot of advances in the following few years. (Large-vocabulary NMT, application to Image captioning, Subword-NMT, Multilingual NMT, Multi-Source NMT, Character-dec NMT, Zero-Resource NMT, Google, Fully Character-NMT, Zero-Shot NMT in 2017) In 2015 there was the first appearance of a NMT system in a public machine translation competition (OpenMT'15). WMT'15 also for the first time had a NMT contender; the following year it already had 90% of NMT systems among its winners.<ref name="WMT16"/>

Workings
NMT departs from phrase-based statistical machine translation|statistical approaches that use separately engineered subcomponents.<ref name="Medical"/> Neural machine translation (NMT) is not a drastic step beyond what has been traditionally done in statistical machine translation (SMT). Its main departure is the use of vector representations ("embeddings", "continuous space representations") for words and internal states. The structure of the models is simpler than phrase-based models. There is no separate language model, translation model, and reordering model, but just a single sequence model that predicts one word at a time. However, this sequence prediction is conditioned on the entire source sentence and the entire already produced target sequence.<ref name="Koehn"/>

NMT models use deep learning and representation learning.

The word sequence modeling was at first typically done using a recurrent neural network (RNN).
A bidirectional recurrent neural network, known as an ''encoder'', is used by the neural network to encode a source sentence for a second RNN, known as a ''decoder'', that is used to predict words in the target language (translation)|target language.<ref name="align&translate"/>

Convolutional Neural Networks (Convnets) are in principle somewhat better for long continuous sequences, but were initially not used due to several weaknesses that were successfully compensated for by 2017 by using so-called "attention"-based approaches.<ref name="attention"/><ref name="DeepL"/>

Usage
By 2016, most of the best MT systems were using neural networks.<ref name="WMT16"/>
Google Translator|Google, Microsoft Translator|Microsoft and Yandex.Translate|Yandex<ref name="Yandex"/> translation services now use NMT. Google uses Google Neural Machine Translation (GNMT) in preference to its previous statistical methods.<ref name="AIawakening"/> Microsoft uses a similar technology for its speech translations (including Microsoft Translator live and Skype Translator).<ref name="MS-NMT"/> An Open-source license|open source neural machine translation system, OpenNMT, has been released by the Harvard NLP group.<ref name="OpenNMT"/> Yandex.Translator has a hybrid model: its translation provides a statistical model and a neural network. After this, the algorithm CatBoost, which is based on machine learning, will select the best of the obtained results<ref name="Yandex"/> Machine translation providers who also offer neural machine translation include Pangeanic's language lab PangeaMT,<ref name="PangeaMT">Cite web|url=https://pangeamt.com | title=PangeaMT ! pangeamt.com|website=Machine Translation|language=en|access-date=2018-11-12</ref> Omniscien Technologies (formerly Asia Online),<ref name="Omniscien"/> Tilde,<ref>Cite web|url=https://tilde.com/products-and-services/machine-translation/features/neural-translation|title=MT Features: Neural Machine Translation ! tilde.com|website=Machine Translation|language=en|access-date=2018-02-09</ref> KantanMT,<ref name="KantanMT"/> SDL plc|SDL,<ref name="SDL"/> Globalese,<ref name="Globalese"/> and TransPerfect. DeepL offers a generic machine translation system with deep learning AI systems while Omniscien Technologies provides customized deep neural machine translation (Deep NMT) and SYSTRAN|Systran offers Pure Neural Machine Translation with deep neural networks.

References
reflist|refs=
<ref name="WMT16">cite journal|last1=Bojar|first1=Ondrej|last2=Chatterjee|first2=Rajen|last3=Federmann|first3=Christian|last4=Graham|first4=Yvette|last5=Haddow|first5=Barry|last6=Huck|first6=Matthias|last7=Yepes|first7=Antonio Jimeno|last8=Koehn|first8=Philipp|last9=Logacheva|first9=Varvara|last10=Monz|first10=Christof|last11=Negri|first11=Matteo|last12=Névéol|first12=Aurélie|last13=Neves|first13=Mariana|last14=Popel|first14=Martin|last15=Post|first15=Matt|last16=Rubino|first16=Raphael|last17=Scarton|first17=Carolina|last18=Specia|first18=Lucia|last19=Turchi|first19=Marco|last20=Verspoor|first20=Karin|last21=Zampieri|first21=Marcos|title=Findings of the 2016 Conference on Machine Translation|journal=ACL 2016 First Conference on Machine Translation (WMT16)|date=2016|pages=131–198|url=https://cris.fbk.eu/retrieve/handle/11582/307240/14326/W16-2301.pdf|publisher=The Association for Computational Linguistics</ref>

<ref name="Medical">cite journal |last1=Wołk |first1=Krzysztof |last2=Marasek |first2=Krzysztof |title=Neural-based Machine Translation for Medical Text Domain. Based on European Medicines Agency Leaflet Texts |year=2015 |url= |journal=Procedia Computer Science |volume=64 |issue=64 |pages=2–9 |doi=10.1016/j.procs.2015.08.456</ref>

<ref name="Koehn">Cite news|url=https://omniscien.com/state-neural-machine-translation-nmt/|title=The State of Neural Machine Translation (NMT)|author=Philipp Koehn|date=2016-11-30|publisher=Omniscien Technologies|access-date=2017-11-08</ref>

<ref name="attention">Cite arxiv|last=Bahdanau|first=Dzmitry|last2=Cho|first2=Kyunghyun|last3=Bengio|first3=Yoshua|date=2014-09-01|title=Neural Machine Translation by Jointly Learning to Align and Translate|eprint=1409.0473|class=cs.CL</ref>

<ref name="DeepL">Cite news|url=https://techcrunch.com/2017/08/29/deepl-schools-other-online-translators-with-clever-machine-learning/|title=DeepL schools other online translators with clever machine learning|last=Coldewey|first=Devin|work=TechCrunch|date=2017-08-29|access-date=2018-01-27</ref>

<ref name="AIawakening">cite news |publisher=The New York Times |first=Gideon|last=Lewis-Kraus|title=The Great A.I. Awakening |url=https://www.nytimes.com/2016/12/14/magazine/the-great-ai-awakening.html |date=December 14, 2016 |access-date=2016-12-21</ref>

<ref name="MS-NMT">Cite news|url=https://blogs.msdn.microsoft.com/translation/2016/11/15/microsoft-translator-launching-neural-network-based-translations-for-all-its-speech-languages/|title=Microsoft Translator launching Neural Network based translations for all its speech languages|work=Translator|access-date=2018-01-27</ref>

<ref name="OpenNMT">Cite web|url=http://opennmt.net/|title=OpenNMT – Open-Source Neural Machine Translation|website=opennmt.net|access-date=2017-03-22</ref>

<ref name="Yandex">Cite news|url=https://yandex.com/company/blog/one-model-is-better-than-two-yu-yandex-translate-launches-a-hybrid-machine-translation-system/|title=Yandex — Company blog — One model is better than two. Yandex.Translate launches a hybrid machine translation system|work=Yandex|access-date=2018-01-27</ref>

<ref name="Omniscien">Cite news|url=https://omniscien.com/omniscien-technologies-announces-release-language-studio-next-generation-neural-machine-translation-technology/|title=Omniscien Technologies Announces Release of Language Studio with Next-Generation Neural Machine Translation Technology|date=2017-04-21|publisher=Omniscien Technologies|access-date=2017-11-08</ref>

<ref name="KantanMT">Cite web|url=https://www.kantanmt.com/displayarticle.php?id=WMA-kycAAP8GvykS&slug=kantanmt-users-can-now-customise-and-deploy-neural-machine-translation-engines|title=KantanMT – Cloud-based Machine Translation Platform|website=KantanMT|access-date=2017-11-08</ref>

<ref name="SDL">Cite web|url=http://www.sdl.com/about/news-media/press/2017/sdl-brings-nmt-to-its-secure-ets.html|title=SDL Brings Neural Machine Translation to its Secure Enterprise Translation Server|website=SDL|access-date=2017-11-08</ref>

<ref name="Globalese">Cite news|url=http://www.globalese-mt.com/2017/09/05/globalese-3-0-released/|title=Globalese 3.0 released|date=2017-09-05|work=Globalese|access-date=2017-11-08</ref>

<ref name="KalchbrennerBlunsom">cite journal|last1=Kalchbrenner|first1=Nal|last2=Blunsom|first2=Philip|title=Recurrent Continuous Translation Models|journal=Proceedings of the Association for Computational Linguistics|date=2013|url=http://www.aclweb.org/anthology/D13-1176</ref>

<ref name="sequence">cite arxiv|last1=Sutskever|first1=Ilya|last2=Vinyals|first2=Oriol|last3=Le|first3=Quoc Viet|title=Sequence to sequence learning with neural networks|eprint=1409.3215|class=cs.CL|year=2014</ref>

<ref name="Properties">cite arXiv | eprint = 1409.1259|author1=Kyunghyun Cho |author2=Bart van Merrienboer |author3=Dzmitry Bahdanau |author4=Yoshua Bengio | title = On the Properties of Neural Machine Translation: Encoder–Decoder Approaches | date = 3 September 2014 | class = cs.CL </ref>

<ref name="layers">Cite news|url=https://omniscien.com/deep-neural-machine-translation/|title=Deep Neural Machine Translation|publisher=Omniscien Technologies|access-date=2017-11-08</ref>

<ref name="align&translate">cite arXiv | eprint = 1409.0473|author1=Dzmitry Bahdanau |author2=Cho Kyunghyun |author3=Yoshua Bengio | title = Neural Machine Translation by Jointly Learning to Align and Translate | year = 2014 | class = cs.CL </ref>


Approaches to machine translation

Category:Artificial intelligence applications
Category:Computational linguistics
Category:Machine translation
Category:Tasks of natural language processing