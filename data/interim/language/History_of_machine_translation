Use dmy dates|date=September 2017
Machine translation is a sub-field of computational linguistics that investigates the use of software to translate text or speech from one natural language to another.

In the 1950s, machine translation became a reality in research, although references to the subject can be found as early as the 17th century (Machine translation). The Georgetown-IBM experiment|Georgetown experiment, which involved successful fully automatic translation of more than sixty Russian sentences into English in 1954, was one of the earliest recorded projects.<ref name="Nye">cite journal|last1=Nye|first1=Mary Jo|title=Speaking in Tongues: Science's centuries-long hunt for a common language|journal=Distillations|date=2016|volume=2|issue=1|pages=40–43|url=https://www.sciencehistory.org/distillations/magazine/speaking-in-tongues|accessdate=22 March 2018</ref><ref name="Babel">cite book|last1=Gordin|first1=Michael D.|title=Scientific Babel: How Science Was Done Before and After Global English|date=2015|publisher=University of Chicago Press|location=Chicago, Illinois|isbn=9780226000299</ref> Researchers of the Georgetown experiment asserted their belief that machine translation would be a solved problem within three to five years.<ref name=nutshell>cite web|author=Hutchins, J.|year=2005|url=http://www.hutchinsweb.me.uk/Nutshell-2005.pdf|title=The history of machine translation in a nutshellself-published source|date=December 2013</ref> In the Soviet Union, similar experiments were performed shortly after.<ref>cite thesis | last=Madsen | first=Mathias Winther | title=The Limits of Machine Translation | date=23 December 2009 | publisher=University of Copenhagen | page=11</ref> 
Consequently, the success of the experiment ushered in an era of significant funding for machine translation research in the United States. The achieved progress was much slower than expected; in 1966, the ALPAC|ALPAC report found that ten years of research had not fulfilled the expectations of the Georgetown experiment and resulted in dramatically reduced fundingcitation needed|date=January 2015.

Interest grew in statistical machine translation|statistical models for machine translation, which became more common and also less expensive in the 1980s as available computational power increased.

Although there exists no autonomous system of "fully automatic high quality translation of unrestricted text,"<ref>cite book | last=Melby | first=Alan K. | title=The Possibility of Language | location=Amsterdam |  date=1995 | publisher=J. Benjamins | isbn=9027216142 | pages=27–41 </ref><ref>cite web|url=http://tandibusiness.blogspot.com/2006/02/simple-model-outlining-translation.html |author=Wooten, Adam |title=A Simple Model Outlining Translation Technology |work=T&I Business |date=14 February 2006 |archiveurl=https://archive.is/20120716095630/http://tandibusiness.blogspot.de/2006/02/simple-model-outlining-translation.html |archivedate=16 July 2012 |deadurl=yes |df=dmy-all </ref><ref>cite web|url=http://www.mt-archive.info/Bar-Hillel-1960-App3.pdf|title=Appendix III of 'The present status of automatic translation of languages'|work=Advances in Computers|volume=1|year=1960|pages=158–163 Reprinted in cite book|author=Y.Bar-Hillel|title=Language and information | location= Massachusetts |publisher=Addison-Wesley|year=1964|pages=174–179</ref> there are many programs now available that are capable of providing useful output within strict constraints. Several of these programs are available online, such as Google Translate and the SYSTRAN system that powers AltaVista's BabelFish (now Yahoo's Babelfish as of 9 May 2008).

The beginning
In the mid-1930s the first patents for "translating machines" were applied for by Georges Artsrouni, for an automatic bilingual dictionary using paper tape. Russian Peter Troyanskii submitted a more detailed proposal<ref>cite book|last1=別所|first1=照彦|last2=棚橋|first2=善照|editor1-last=玉木|editor1-first=英彦|editor2-last=喜安|editor2-first=善市|title=自動翻訳　デ・ユ・パノフ著|date=15 October 1960|publisher=（株）みすず書房|location=Tokyo|pages=10–11|edition=1|language=Japanese|chapter=自動翻訳|quote=翻訳のある程度の機械化は1933年にペ・ペ・トロヤンスキーが企てたのがおそらく最初であろう。彼は「一つの言語から他の一つまたは同時に多数の言語への翻訳に際し、単語を選別しかつ印刷する機械」をつくることを提案した。この発明でペ・ペ・トロヤンスキーは特許をとったが、当時それを実現することは巧くいかなかった。(translation (assisted by Google translate): It may be almost first case of machine translation that Peter Petrovich Troyanskii tried in 1933. He presented that "to criate the machine which choice words and print them on translation from a language to another language or to multiple languages simultaneously." He got the patent by this invention, but it was not able to implement at that time.)</ref><ref>cite book|last1=別所|first1=照彦|last2=沢辺|first2=弘|title=翻訳機械　（文庫クセジュ　現代知識の焦点）|date=25 February 1964|publisher=（株）白水社|location=Tokyo|page=39|edition=1|language=Japanese|quote=モスクワで1933年に特許をとったロシア人スミルノフ・トロヤンスキーの発明は、同時にいくつかの言語を翻訳し、遠方まで送ることを可能とするように見えた。(translation (assisted by Google translate): The invention patented by Peter Petrovich Troyanskii in 1933 seemed be able to translate multiple language simultaneously and sent them to far place.), translated from<br />cite book|last1=Delaveney|first1=Émile|title=LA MACHINE A TRADUIRE (Collection QUE SAIS-JE? No.834)|publisher=Presses Universitaires de France|language=French</ref> that included both the bilingual dictionary and a method for dealing with grammatical roles between languages, based on the grammatical system of Esperanto. This system was separated into three stages: stage one consisted of a native-speaking editor in the source language to organize the words into their logical forms and to exercise the syntactic functions; stage two required the machine to "translate" these forms into the target language; and stage three required a native-speaking editor in the target language to normalize this output.  Troyanskii's proposal remained unknown until the late 1950s, by which time computers were well-known and utilized.

The early years
The first set of proposals for computer based machine translation was presented in 1949 by Warren Weaver, a researcher at the Rockefeller Foundation,  "Warren Weaver#The .22Translation.22 memorandum|Translation memorandum".<ref>cite web|url=http://ourworld.compuserve.com/homepages/WJHutchins/Weaver49.htm|title=Weaver memorandum|date=March 1949|archiveurl=https://web.archive.org/web/20061005232830/http://ourworld.compuserve.com/homepages/WJHutchins/Weaver49.htm|archivedate=5 October 2006</ref> These proposals were based on information theory, successes in cryptography|code breaking during the Second World War, and theories about the universal principles underlying natural language.

A few years after Weaver submitted his proposals, research began in earnest at many universities in the United States. On 7 January 1954 the Georgetown-IBM experiment was held in New York at the head office of IBM. This was the first public demonstration of a machine translation system. The demonstration was widely reported in the newspapers and garnered public interest. The system itself, however, was no more than a "toy" system. It had only 250 words and translated 49 carefully selected Russian sentences into English – mainly in the field of chemistry. Nevertheless, it encouraged the idea that machine translation was imminent and stimulated the financing of the research, not only in the US but worldwide.<ref name=nutshell />

Early systems used large bilingual dictionaries and hand-coded rules for fixing the word order in the final output which was eventually considered too restrictive in linguistic developments at the time. For example, generative linguistics and transformational grammar were exploited to improve the quality of translations. During this period operational systems were installed. The United States Air Force used a system produced by IBM and Washington University, while the United States Atomic Energy Commission|Atomic Energy Commission and Euratom, in Italy, used a system developed at Georgetown University. While the quality of the output was poor it met many of the customers' needs, particularly in terms of speed.citation needed|date=January 2015

At the end of the 1950s, Yehoshua Bar-Hillel was asked by the US government to look into machine translation, to assess the possibility of fully automatic high quality translation by machines.  Bar-Hillel described the problem of semantic ambiguity or double-meaning, as illustrated in the following sentence:

quote|Little John was looking for his toy box. Finally he found it. The box was in the pen.

The word ''pen'' may have two meanings: the first meaning, something used to write in ink with; the second meaning, a container of some kind. To a human, the meaning is obvious, but Bar-Hillel claimed that without a "universal encyclopedia" a machine would never be able to deal with this problem. At the time, this type of semantic ambiguity could only be solved by writing source texts for machine translation in a controlled natural language|controlled language that uses a vocabulary in which each word has exactly one meaning.citation needed|date=January 2015

The 1960s, the ALPAC report and the seventies
Research in the 1960s in both the Soviet Union and the United States concentrated mainly on the Russian–English language pair. The objects of translation were chiefly scientific and technical documents, such as articles from scientific journals. The rough translations produced were sufficient to get a basic understanding of the articles. If an article discussed a subject deemed to be confidential, it was sent to a human translator for a complete translation; if not, it was discarded.

A great blow came to machine-translation research in 1966 with the publication of the ALPAC report. The report was commissioned by the US government and delivered by ALPAC, the Automatic Language Processing Advisory Committee, a group of seven scientists convened by the US government in 1964. The US government was concerned that there was a lack of progress being made despite significant expenditure. The report concluded that machine translation was more expensive, less accurate and slower than human translation, and that despite the expenditures, machine translation was not likely to reach the quality of a human translator in the near future.

The report recommended, however, that tools be developed to aid translators – automatic dictionaries, for example – and that some research in computational linguistics should continue to be supported.

The publication of the report had a profound impact on research into machine translation in the United States, and to a lesser extent the Soviet Union and United Kingdom. Research, at least in the US, was almost completely abandoned for over a decade. In Canada, France and Germany, however, research continued.  In the US the main exceptions were the founders of Systran (Peter Toma) and OpenLogos|Logos (Bernard Scott), who established their companies in 1968 and 1970 respectively and served the US Department of Defense.  In 1970, the Systran system was installed for the United States Air Force, and subsequently by the Commission of the European Communities in 1976. The METEO System, developed at the Université de Montréal, was installed in Canada in 1977 to translate weather forecasts from English to French, and was translating close to 80,000 words per day or 30 million words per year until it was replaced by a competitor's system on 30 September 2001.<ref>cite web|url=http://www.citt.gc.ca/procure/determin/pr2b029_e.asp#P90_10741|title=PROCUREMENT PROCESS|work=Canadian International Trade Tribunal|date=30 July 2002|accessdate=10 February 2007|archiveurl=https://web.archive.org/web/20110706181412/http://www.citt.gc.ca/procure/determin/pr2b029_e.asp#P90_10741|archivedate=6 July 2011</ref>

While research in the 1960s concentrated on limited language pairs and input, demand in the 1970s was for low-cost systems that could translate a range of technical and commercial documents. This demand was spurred by the increase of globalisation and the demand for translation in Canada, Europe, and Japan.citation needed|date=January 2015

The 1980s and early 1990s
By the 1980s, both the diversity and the number of installed systems for machine translation had increased. A number of systems relying on Mainframe computer|mainframe technology were in use, such as Systran, OpenLogos|Logos, Ariane-G5, and METAL MT|Metal.citation needed|date=January 2015

As a result of the improved availability of microcomputers, there was a market for lower-end machine translation systems. Many companies took advantage of this in Europe, Japan, and the USA. Systems were also brought onto the market in China, Eastern Europe, Korea, and the Soviet Union.citation needed|date=January 2015

During the 1980s there was a lot of activity in MT in Japan especially.  With the fifth generation computer Japan intended to leap over its competition in computer hardware and software, and one project that many large Japanese electronics firms found themselves involved in was creating software for translating into and from English (Fujitsu, Toshiba, NTT, Brother, Catena, Matsushita, Mitsubishi, Sharp, Sanyo, Hitachi, NEC, Panasonic, Kodensha, Nova, Oki).citation needed|date=January 2015

Research during the 1980s typically relied on translation through some variety of intermediary linguistic representation involving morphological, syntactic, and semantic analysis.citation needed|date=January 2015

At the end of the 1980s, there was a large surge in a number of novel methods for machine translation. One system was developed at IBM that was based on statistical methods. Makoto Nagao and his group used methods based on large numbers of translation examples, a technique that is now termed example-based machine translation.<ref>cite conference | last=Nagao | first=Makoto | title=A Framework of a Mechanical Translation Between Japanese and English by Analogy Principle | booktitle=Procedures of the International NATO Symposium on Artificial and Human Intelligence | location=New York | date=1984 | publisher=Elsevier North-Holland, Inc. | pages=173–180 | isbn=0-444-86545-4 | url=http://mt-archive.info/Nagao-1984.pdf</ref><ref>cite web | url = http://www.aclweb.org/index.php?option=com_content&task=view&id=36&Itemid=30 | title = the Association for Computational Linguistics – 2003 ACL Lifetime Achievement Award | publisher = Association for Computational Linguistics | accessdate = 10 March 2010</ref> A defining feature of both of these approaches was the neglect of syntactic and semantic rules and reliance instead on the manipulation of large text Corpus linguistics|corpora.

During the 1990s, encouraged by successes in speech recognition and speech synthesis, research began into speech translation with the development of the German Verbmobil project.

The Forward Area Language Converter (FALCon) system, a machine translation technology designed by the United States Army Research Laboratory|Army Research Laboratory, was fielded 1997 to translate documents for soldiers in Bosnia.<ref>Cite book|url=https://books.google.com/books?id=wcFrCQAAQBAJ&pg=PA197&lpg=PA197&dq=Forward+Area+Language+Converter+(FALCon)+systems&source=bl&ots=tEfCtI_pvZ&sig=CmCBjwJpfSMjR1c3RLMmP_Hylsk&hl=en&sa=X&ved=0ahUKEwijjJWqsM7bAhUE0VMKHRMWDGIQ6AEIKzAB#v=onepage&q=Forward%20Area%20Language%20Converter%20(FALCon)%20systems&f=false|title=Envisioning Machine Translation in the Information Future: 4th Conference of the Association for Machine Translation in the Americas, AMTA 2000, Cuernavaca, Mexico, October 10-14, 2000 Proceedings|last=White|first=John S.|date=2003-07-31|publisher=Springer|isbn=9783540399650|language=en</ref> 

There was significant growth in the use of machine translation as a result of the advent of low-cost and more powerful computers.  It was in the early 1990s that machine translation began to make the transition away from large mainframe computers toward personal computers and workstations.  Two companies that led the PC market for a time were Globalink and MicroTac, following which a merger of the two companies (in December 1994) was found to be in the corporate interest of both. Intergraph and Systran also began to offer PC versions around this time. Sites also became available on the internet, such as AltaVista's Babel Fish (website)|Babel Fish (using Systran technology) and Google Google tools#anchor language tools|Language Tools (also initially using Systran technology exclusively).

2000s
The field of machine translation has seen major changes in the last few years. Currently a large amount of research is being done into statistical machine translation and example-based machine translation. 
In the area of speech translation, research has focused on moving from domain-limited systems to domain-unlimited translation systems. In different research projects in Europe (like TC-STAR)<ref>cite web | url = http://www.tcstar.org| title =TC-Star  | accessdate = 25 October 2010</ref> and in the United States (STR-DUST and US-DARPA-GALE),<ref>Cite web | url = http://www.darpa.mil/ipto/programs/gale/gale.asp | title =U.S.-DARPA-GALE  | accessdate = 25 October 2010</ref> solutions for automatically translating Parliamentary speeches and broadcast news have been developed. In these scenarios the domain of the content is no longer limited to any special area, but rather the speeches to be translated cover a variety of topics.
More recently, the French-German project Quaero investigates the possibility of making use of machine translations for a multi-lingual internet. The project seeks to translate not only webpages, but also videos and audio files on the internet.

Today, only a few companies use statistical machine translation commercially, e.g. Omniscien Technologies (formerly Asia Online),citation needed|date=January 2015 SDL plc|SDL / Language Weaver (sells translation products and services),citation needed|date=January 2015 Google (uses its proprietary statistical MT system for some language combinations in Google's language tools),<ref>cite web | url = https://googlesystem.blogspot.de/2007/10/google-translate-switches-to-googles.html| title =Google Switches to Its Own Translation System | accessdate = 12 February 2018</ref> Microsoft (uses its proprietary statistical MT system to translate knowledge base articles),citation needed|date=January 2015 and Ta with you (offers a domain-adapted machine translation solution based on statistical MT with some linguistic knowledge).citation needed|date=January 2015 There has been a renewed interest in hybridisation, with researchers combining syntactic and morphological (i.e., linguistic) knowledge into statistical systems, as well as combining statistics with existing rule-based systems.citation needed|date=January 2015

See also
* History of natural language processing
* ALPAC report
* Computer-aided translation
* Lighthill report
* Machine translation

Notes
reflist

References
*cite web|author=Hutchins, J.|year=2005|url=http://www.mt-archive.info/Weaver-1949.pdf|title=Milestones in machine translation – No.6: Bar-Hillel and the nonfeasibility of FAHQT]
*cite book | last=Van Slype | first=Georges | title=Better translation for better communication | location=Paris | date=1983 | publisher=Pergamon Press | isbn=9780080305349

Further reading
*cite book | last=Hutchins | first=W. John | title=Machine Translation: past, present, future | location=Chichester | series=Ellis Horwood series in computers and their applications | date=1986 | publisher=Ellis Horwood | isbn=0470203137 | url=http://www.hutchinsweb.me.uk/PPF-TOC.htm her

DEFAULTSORT:History Of Machine Translation
Category:Machine translation
Category:History of artificial intelligence
Category:Natural language processing
Category:History of linguistics
Category:History of software