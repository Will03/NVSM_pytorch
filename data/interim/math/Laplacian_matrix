In the mathematics|mathematical field of graph theory, the Laplacian matrix, sometimes called admittance matrix, Kirchhoff matrix or discrete Laplacian, is a matrix (mathematics)|matrix representation of a Graph (discrete mathematics)|graph. The Laplacian matrix can be used to find many useful properties of a graph. Together with Kirchhoff's theorem, it can be used to calculate the number of spanning tree (mathematics)|spanning trees for a given graph.  The cut (graph theory)#Sparsest cut|sparsest cut of a graph can be approximated through the second smallest eigenvalue of its Laplacian by Cheeger constant#Cheeger's inequality|Cheeger's inequality. It can also be used to construct nonlinear dimensionality reduction#Laplacian eigenmaps|low dimensional embeddings, which can be useful for a variety of machine learning applications.

 Definition 

= Laplacian matrix for ''simple graphs'' =

Given a simple graph ''G'' with ''n'' vertices, its Laplacian matrix <math display="inline">L_{n \times n}</math> is 
defined as:<ref name="mathworld">MathWorld |urlname=LaplacianMatrix |title=Laplacian Matrix</ref>
: <math>L = D - A,</math>

where ''D'' is the degree matrix and ''A'' is the adjacency matrix of the graph. Since <math display="inline">G</math> is a simple graph, <math display="inline">A</math> only contains 1s or 0s and its diagonal elements are all 0s.

In the case of directed graphs, either the degree (graph theory)|indegree or outdegree might be used, depending on the application.

The elements of <math display="inline">L</math> are given by

: <math>L_{i,j} := \begin{cases}
  \deg(v_i) & \mbox{if}\ i = j \\
         -1 & \mbox{if}\ i \neq j\ \mbox{and}\ v_i \mbox{ is adjacent to } v_j \\
          0 & \mbox{otherwise}
\end{cases}</math>

where deg(''v<sub>i</sub>'') is the degree of the vertex ''i''.

 Symmetric normalized Laplacian 

The symmetric normalized Laplacian matrix is defined  as:<ref name="mathworld" />

: <math>L^\text{sym} := D^{-\frac{1}{2 L D^{-\frac{1}{2 = I - D^{-\frac{1}{2 A D^{-\frac{1}{2</math>,

The elements of <math display="inline">L^\text{sym}</math> are given by

:<math>L^\text{sym}_{i,j} := \begin{cases}
                                     1 & \mbox{if } i = j \mbox{ and } \deg(v_i) \neq 0\\
  -\frac{1}{\sqrt{\deg(v_i)\deg(v_j) & \mbox{if } i \neq j \mbox{ and } v_i \mbox{ is adjacent to } v_j \\
                                     0 & \mbox{otherwise}.
\end{cases}</math>

 Random walk normalized Laplacian 

The random-walk normalized Laplacian matrix is defined as:
: <math>L^\text{rw} := D^{-1}L = I - D^{-1}A</math>

The elements of <math display="inline">L^\text{rw}</math> are given by
:<math>L^\text{rw}_{i,j} := \begin{cases}
                     1 & \mbox{if } i = j \mbox{ and } \deg(v_i) \neq 0\\
  -\frac{1}{\deg(v_i)} & \mbox{if } i \neq j \mbox{ and } v_i \mbox{ is adjacent to } v_j \\
                     0 & \mbox{otherwise}.
\end{cases}</math>

 Generalized Laplacian 

The generalized Laplacian, ''Q'', is defined as<ref>cite book |last1= Godsil |first1=C. |last2= Royle |first2=G. |date=2001 |title=Algebraic Graph Theory, Graduate Texts in Mathematics |publisher= Springer-Verlag</ref>:

: <math>\begin{cases}
        Q_{i,j} < 0 & \mbox{if } i \neq j \mbox{ and } v_i \mbox{ is adjacent to } v_j\\
        Q_{i,j} = 0 & \mbox{if } i \neq j \mbox{ and } v_i \mbox{ is not adjacent to } v_j \\
  \mbox{any number} & \mbox{otherwise}.
\end{cases}</math>

Notice the ordinary Laplacian is a generalized Laplacian.

 Example 

Here is a simple example of a labeled, undirected graph and its Laplacian matrix.

{|class="wikitable"
! Labeled graph
! Degree matrix
! Adjacency matrix
! Laplacian matrix
|-
| image:6n-graf.svg|175px
| <math display="inline">\left(\begin{array}{rrrrrr}
  2 &  0 &  0 &  0 &  0 &  0\\
  0 &  3 &  0 &  0 &  0 &  0\\
  0 &  0 &  2 &  0 &  0 &  0\\
  0 &  0 &  0 &  3 &  0 &  0\\
  0 &  0 &  0 &  0 &  3 &  0\\
  0 &  0 &  0 &  0 &  0 &  1\\
\end{array}\right)</math>
| <math display="inline">\left(\begin{array}{rrrrrr}
  0 &  1 &  0 &  0 &  1 &  0\\
  1 &  0 &  1 &  0 &  1 &  0\\
  0 &  1 &  0 &  1 &  0 &  0\\
  0 &  0 &  1 &  0 &  1 &  1\\
  1 &  1 &  0 &  1 &  0 &  0\\
  0 &  0 &  0 &  1 &  0 &  0\\
\end{array}\right)</math>
| <math display="inline">\left(\begin{array}{rrrrrr}
   2 & -1 &  0 &  0 & -1 &  0\\
  -1 &  3 & -1 &  0 & -1 &  0\\
   0 & -1 &  2 & -1 &  0 &  0\\
   0 &  0 & -1 &  3 & -1 & -1\\
  -1 & -1 &  0 & -1 &  3 &  0\\
   0 &  0 &  0 & -1 &  0 &  1\\
\end{array}\right)</math>
|}

 Properties 
For an (undirected) graph ''G'' and its Laplacian matrix ''L'' with eigenvalues <math display="inline">\lambda_0 \le \lambda_1 \le \cdots \le \lambda_{n-1}</math>:

* ''L'' is symmetric.
* ''L'' is positive-definite matrix|positive-semidefinite (that is <math display="inline">\lambda_i \ge 0</math> for all <math display="inline">i</math>). This is verified in the #Incidence matrix|incidence matrix section (below). This can also be seen from the fact that the Laplacian is symmetric and diagonally dominant matrix#Applications and properties|diagonally dominant.
* ''L'' is an M-matrix (its off-diagonal entries are nonpositive, yet the real parts of its eigenvalues are nonnegative).
* Every row sum and column sum of ''L'' is zero. Indeed, in the sum, the degree of the vertex is summed with a "−1" for each neighbor.
* In consequence, <math display="inline">\lambda_0 = 0</math>, because the vector <math display="inline">\mathbf{v}_0 = (1, 1, \dots, 1)</math> satisfies <math display="inline">L \mathbf{v}_0 = \mathbf{0} .</math> This also implies that the Laplacian matrix is singular.
* The number of Connected component (graph theory)|connected components in the graph is the dimension of the kernel (linear algebra)|nullspace of the Laplacian and the Eigenvalues and eigenvectors#Algebraic multiplicity|algebraic multiplicity of the 0 eigenvalue.
* The smallest non-zero eigenvalue of ''L'' is called the spectral gap.
* The second smallest eigenvalue of ''L'' (could be zero) is the algebraic connectivity (or Fiedler value) of ''G'' and approximates the cut (graph_theory)#Sparsest cut|sparsest cut of a graph.
* The Laplacian is an operator on the n-dimensional vector space of functions <math display="inline">f : V \to \mathbb{R}</math>, where <math display="inline">V</math> is the vertex set of G, and <math display="inline">n = |V|</math>.
* When G is k-regular, the normalized Laplacian is: <math display="inline">\mathcal{L} = \tfrac{1}{k} L = I - \tfrac{1}{k} A</math>, where A is the adjacency matrix and I is an identity matrix.
* For a graph with multiple Connected component (graph theory)|connected components, ''L'' is a Block matrix#Block diagonal matrices|block diagonal matrix, where each block is the respective Laplacian matrix for each component, possibly after reordering the vertices (i.e. ''L'' is permutation-similar to a block diagonal matrix).
* The trace of the Laplacian matrix ''L'' is equal to <math display="inline">2m</math> where <math display="inline">m</math> is the number of edges of the considered graph.

 Incidence matrix 

Define an <math display="inline">|e| \times |v|</math> oriented incidence matrix ''M'' with element ''M''<sub>''ev''</sub> for edge ''e'' (connecting vertex ''i'' and ''j'', with ''i''&nbsp;>&nbsp;''j'') and vertex ''v'' given by
:<math>M_{ev} = \left\{\begin{array}{rl}
   1, & \text{if } v = i\\
  -1, & \text{if } v = j\\
   0, & \text{otherwise}.
\end{array}\right.</math>

Then the Laplacian matrix ''L'' satisfies
:<math>L = M^\textsf{T} M\,,</math>

where <math display="inline">M^\textsf{T}</math> is the transpose|matrix transpose of ''M''.

Now consider an eigendecomposition of <math display="inline">L</math>, with unit-norm eigenvectors <math display="inline">\mathbf{v}_i</math> and corresponding eigenvalues <math display="inline">\lambda_i</math>:
:<math>\begin{align}
  \lambda_i & = \mathbf{v}_i^\textsf{T} L \mathbf{v}_i \\
            & = \mathbf{v}_i^\textsf{T} M^\textsf{T} M \mathbf{v}_i \\
            & = \left(M \mathbf{v}_i\right)^\textsf{T} \left(M \mathbf{v}_i\right). \\
\end{align}</math>

Because <math display="inline">\lambda_i</math> can be written as the inner product of the vector <math display="inline">M \mathbf{v}_i</math> with itself, this shows that <math display="inline">\lambda_i \ge 0</math> and so the eigenvalues of <math display="inline">L</math> are all non-negative.

 Deformed Laplacian 

The deformed Laplacian is commonly defined as

:<math>\Delta(s) = I - sA + s^2(D - I)</math>

where ''I'' is the unit matrix, ''A'' is the adjacency matrix, and ''D'' is the degree matrix, and ''s'' is a (complex-valued) number.  Note that the standard Laplacian is just <math display="inline">\Delta(1)</math>.<ref>cite journal |title=The Deformed Consensus Protocol |first=F. |last=Morbidi |journal=Automatica |volume=49 |number=10 |pages=3049–3055 |year=2013 |doi=10.1016/j.automatica.2013.07.006</ref>

 Symmetric normalized Laplacian 

The (symmetric) normalized Laplacian is defined as
: <math>L^\text{sym} := D^{-\frac{1}{2 L D^{-\frac{1}{2 = I - D^{-\frac{1}{2AD^{-\frac{1}{2</math>

where ''L'' is the (unnormalized) Laplacian, ''A'' is the adjacency matrix and ''D'' is the degree matrix. Since the degree matrix ''D'' is diagonal and positive, its reciprocal square root <math display="inline">D^{-\frac{1}{2</math> is just the diagonal matrix whose diagonal entries are the reciprocals of the positive square roots of the diagonal entries of ''D''. The symmetric normalized Laplacian is a symmetric matrix.

One has: <math display="inline">L^\text{sym} = S S^*</math>, where S is the matrix whose rows are indexed by the vertices and whose columns are indexed by the edges of G such that each column corresponding to an edge e = {u, v} has an entry <math display="inline">\frac{1}{\sqrt{d_u</math> in the row corresponding to u, an entry <math display="inline">-\frac{1}{\sqrt{d_v</math> in the row corresponding to v, and has 0 entries elsewhere. (Note: <math display="inline">S^*</math> denotes the transpose of S).

All eigenvalues of the normalized Laplacian are real and non-negative. We can see this as follows. Since <math display="inline">L^\text{sym}</math> is symmetric, its eigenvalues are real. They are also non-negative: consider an eigenvector <math display="inline">g</math> of <math display="inline">L^\text{sym}</math> with eigenvalue &lambda; and suppose <math display="inline">g = D^\frac{1}{2} f</math>. (We can consider g and f as real functions on the vertices v.) Then:

:<math>
  \lambda \ = \ 
  \frac{\langle g, L^\text{sym}g\rangle}{\langle g, g\rangle} \ = \ 
  \frac{\left\langle g, D^{-\frac{1}{2 L D^{-\frac{1}{2 g\right\rangle}{\langle g, g\rangle} \ = \ 
  \frac{\langle f, Lf\rangle}{\left\langle D^\frac{1}{2} f, D^\frac{1}{2} f\right\rangle} \  = \ 
  \frac{\sum_{u \sim v}(f(u) - f(v))^2}{\sum_v f(v)^2 d_v} \ \geq \ 
  0,
</math>

where we use the inner product <math display="inline">\langle f,g\rangle = \sum_{v} f(v)g(v)</math>, a sum over all vertices v, and <math display="inline">\sum_{u\sim v}</math> denotes the sum over all unordered pairs  of adjacent vertices {u,v}. The quantity <math display="inline">\sum_{u,v}(f(u) - f(v))^2</math> is called the ''Dirichlet sum'' of f, whereas the expression <math display="inline">\frac{\left\langle g, L^\text{sym}g\right\rangle}{\langle g, g\rangle} </math> is called the ''Rayleigh quotient'' of g.

Let 1 be the function which assumes the value 1 on each vertex. Then <math display="inline">D^\frac{1}{2} 1</math> is an eigenfunction of <math display="inline">L^{\text{sym</math> with eigenvalue 0.<ref>cite book|last=Chung|first=Fan R. K.|authorlink=Fan Chung|title=Spectral graph theory|year=1997|publisher=American Math. Soc.|location=Providence, RI|isbn=0-8218-0315-8|edition=Repr. with corr., 2. [pr.]</ref>

In fact, the eigenvalues of the normalized symmetric Laplacian satisfy 0 = μ<sub>0</sub> ≤ … ≤ μ<sub>n−1</sub> ≤ 2. These eigenvalues (known as the spectrum of the normalized Laplacian) relate well to other graph invariants for general graphs.<ref>cite book
| last                  = Chung
| first                 = Fan
| authorlink            = Fan Chung
| title                 = Spectral Graph Theory
| url                   = http://www.math.ucsd.edu/~fan/research/revised.html
| origyear              = 1992
| year                  = 1997
| publisher             = American Mathematical Society
| isbn                  = 0821803158
</ref>

 Random walk normalized Laplacian 

The random walk normalized Laplacian is defined as
: <math>L^\text{rw} := D^{-1} L</math>

where ''D'' is the degree matrix. Since the degree matrix ''D'' is diagonal, its inverse <math display="inline">D^{-1}</math> is simply defined as a diagonal matrix, having diagonal entries which are the reciprocals of the corresponding positive diagonal entries of ''D''.

For the isolated vertices (those with degree 0), a common choice is to set the corresponding element <math display="inline">L^\text{rw}_{i,i}</math> to 0.

This convention results in a nice property that the multiplicity of the eigenvalue 0 is equal to the number of connected components in the graph.

The matrix elements of <math display="inline">L^\text{rw}</math> are given by
: <math>L^{\text{rw_{i,j} := \begin{cases}
                     1 & \mbox{if}\ i = j\ \mbox{and}\ \deg(v_i) \neq 0\\
  -\frac{1}{\deg(v_i)} & \mbox{if}\ i \neq j\ \mbox{and}\ v_i \mbox{ is adjacent to } v_j \\
                     0 & \mbox{otherwise}.
\end{cases}</math>

The name of the random-walk normalized Laplacian comes from the fact that this matrix is <math display="inline">L^\text{rw} = I - P</math>, where <math display="inline">P = D^{-1}A</math> is simply the transition matrix
of a random walker on the graph. For example, let <math display="inline"> e_i </math> denote the i-th standard basis vector. Then <math display="inline">x = e_i P </math> is a probability vector representing the distribution of a random walker's locations after taking a single step from vertex <math display="inline">i</math>; i.e., <math display="inline">x_j = \mathbb{P}\left(v_i \to v_j\right)</math>. More generally, if the vector <math display="inline"> x </math> is a probability distribution of the location of a random walker on the vertices of the graph, then <math display="inline">x' = x P^t</math> is the probability distribution of the walker after <math display="inline">t</math> steps.

One can check that
: <math>L^\text{rw} = I-D^{-\frac{1}{2\left(I - L^\text{sym}\right) D^\frac{1}{2}</math>,

i.e., <math display="inline">L^\text{rw}</math> is similar to the normalized Laplacian <math display="inline">L^\text{sym}</math>. For this reason, even if <math display="inline">L^\text{rw}</math> is in general not hermitian, it has real eigenvalues. Indeed, its eigenvalues agree with those of <math display="inline">L^\text{sym}</math> (which is hermitian). 

= Graphs =

As an aside about random walk#Random walk on graphs|random walks on graphs, consider a simple graph (discrete mathematics)#Undirected graph|undirected graph. Consider the probability that the walker is at the vertex ''i'' at time ''t'', given the probability distribution that he was at vertex ''j'' at time ''t − 1'' (assuming a uniform chance of taking a step along any of the edges attached to a given vertex):
: <math>p_i(t) = \sum_j \frac{A_{ij{\deg\left(v_j\right)} p_j(t - 1),</math>

or in matrix-vector notation:
:<math>p(t) = A D^{-1} p(t - 1).</math>

(Equilibrium, which sets in as <math display="inline">t\rightarrow \infty</math>, is defined by <math display="inline">p = A D^{-1} p </math>.)

We can rewrite this relation as
:<math>D^{-\frac{1}{2 p(t) = \left[D^{-\frac{1}{2 A D^{-\frac{1}{2\right] D^{-\frac{1}{2 p(t - 1).</math>

<math display="inline">A_\text{reduced} \equiv D^{-\frac{1}{2 A D^{-\frac{1}{2</math> is a symmetric matrix called the reduced adjacency matrix. So, taking steps on this random walk requires taking powers of <math display="inline">A_\text{reduced}</math>, which is a simple operation because <math display="inline">A_\text{reduced}</math> is symmetric.

 Interpretation as the discrete Laplace operator 

The Laplacian matrix can be interpreted as a matrix representation of a particular case of the discrete Laplace operator. Such an interpretation allows one, e.g., to generalise the Laplacian matrix to the case of graphs with an infinite number of vertices and edges, leading to a Laplacian matrix of an infinite size.

Suppose <math display="inline">\phi</math> describes a heat distribution across a graph (discrete mathematics)|graph, where <math display="inline">\phi_i</math> is the heat at vertex <math display="inline">i</math>. According to Newton's law of cooling, the heat transferred between nodes <math display="inline">i</math> and <math display="inline">j</math> is proportional to <math display="inline">\phi_i - \phi_j</math> if nodes <math display="inline">i</math> and <math display="inline">j</math> are connected (if they are not connected, no heat is transferred). Then, for heat capacity <math display="inline">k</math>,

:<math>\begin{align}
  \frac{d \phi_i}{d t}
    &= -k \sum_j A_{ij} \left( \phi_i - \phi_j \right) \\
    &= -k \left( \phi_i \sum_j A_{ij} - \sum_j A_{ij} \phi_j \right) \\
    &= -k \left( \phi_i \ \deg(v_i) - \sum_j A_{ij} \phi_j \right) \\
    &= -k \sum_j \left( \delta_{ij} \ \deg(v_i) - A_{ij} \right) \phi_j \\
    &= -k \sum_j \left( \ell_{ij} \right) \phi_j.
\end{align}</math>

In matrix-vector notation,
:<math>\begin{align}
  \frac{d\phi}{dt} &= -k(D - A)\phi \\
                   &= -kL \phi,
\end{align}</math>

which gives
:<math>\frac{d \phi}{d t} + kL\phi = 0.</math>

Notice that this equation takes the same form as the heat equation, where the matrix −''L'' is replacing the Laplacian operator <math display="inline">\nabla^2</math>; hence, the "graph Laplacian".

To find a solution to this differential equation, apply standard techniques for solving a first-order matrix differential equation.  That is, write <math display="inline">\phi</math> as a linear combination of eigenvectors <math display="inline">\mathbf{v}_i</math> of ''L'' (so that <math display="inline">L\mathbf{v}_i = \lambda_i \mathbf{v}_i</math>), with time-dependent <math display="inline">\phi = \sum_i c_i \mathbf{v}_i.</math>

Plugging into the original expression (note that we will use the fact that because ''L'' is a symmetric matrix, its unit-norm eigenvectors <math display="inline">\mathbf{v}_i</math> are orthogonal):

:<math>\begin{align}
  \frac{d\left(\sum_i c_i \mathbf{v}_i\right)}{dt} + kL\left(\sum_i c_i \mathbf{v}_i\right) &= 0 \\
                    \sum_i \left[\frac{dc_i}{dt} \mathbf{v}_i + k c_i L \mathbf{v}_i\right] &= \\
            \sum_i \left[\frac{dc_i}{dt} \mathbf{v}_i + k c_i \lambda_i \mathbf{v}_i\right] &= \\
                                                          \frac{dc_i}{dt} + k \lambda_i c_i &= 0, \\
\end{align}</math>


whose solution is
:<math>c_i(t) = c_i(0) e^{-k \lambda_i t}.</math>

As shown before, the eigenvalues <math display="inline">\lambda_i</math> of ''L'' are non-negative, showing that the solution to the diffusion equation approaches an equilibrium, because it only exponentially decays or remains constant. This also shows that given <math display="inline">\lambda_i</math> and the initial condition <math display="inline">c_i(0)</math>, the solution at any time ''t'' can be found.<ref>cite book
| last                  = Newman
| first                 = Mark
| authorlink            = Mark Newman
| title                 = Networks: An Introduction
| year                  = 2010
| publisher             = Oxford University Press
| isbn                  = 0199206651
</ref>

To find <math display="inline">c_i(0)</math> for each <math display="inline">i</math> in terms of the overall initial condition <math display="inline">\phi(0)</math>, simply project <math display="inline">\phi(0)</math> onto the unit-norm eigenvectors <math display="inline">\mathbf{v}_i</math>;
: <math>c_i(0) = \left\langle \phi(0), \mathbf{v}_i \right\rangle </math>.

In the case of undirected graphs, this works because <math display="inline">L</math> is symmetric, and by the spectral theorem, its eigenvectors are all orthogonal.  So the projection onto the eigenvectors of <math display="inline">L</math> is simply an orthogonal coordinate transformation of the initial condition to a set of coordinates which decay exponentially and independently of each other.

= Equilibrium behavior =
To understand <math display="inline">\lim_{t \to \infty}\phi(t)</math>, note that the only terms <math display="inline"> c_i(t) = c_i(0) e^{-k \lambda_i t}</math> that remain are those where <math display="inline">\lambda_i = 0</math>, since
: <math>\lim_{t\to\infty} e^{-k \lambda_i t} = \left\{\begin{array}{rlr}
  0 & \text{if} & \lambda_i > 0 \\
  1 & \text{if} & \lambda_i = 0
\end{array}\right\}</math>

In other words, the equilibrium state of the system is determined completely by the Kernel (linear algebra)|kernel of <math display="inline">L</math>.  

Since by definition, <math display="inline">\sum_{j}L_{ij} = 0</math>, the vector <math display="inline">\mathbf{v}^1</math> of all ones is in the kernel.  Note also that if there are <math display="inline">k</math> disjoint Connected component (graph theory)|connected components in the graph, then this vector of all ones can be split into the sum of <math display="inline">k</math> independent <math display="inline">\lambda = 0</math> eigenvectors of ones and zeros, where each connected component corresponds to an eigenvector with ones at the elements in the connected component and zeros elsewhere.

The consequence of this is that for a given initial condition <math display="inline">c(0)</math> for a graph with <math display="inline">N</math> vertices
: <math>\lim_{t\to\infty}\phi(t) = \left\langle c(0), \mathbf{v^1} \right\rangle \mathbf{v^1}</math>

where
: <math>\mathbf{v^1} = \frac{1}{\sqrt{N [1, 1, ..., 1] </math>

For each element <math display="inline">\phi_j</math> of <math display="inline">\phi</math>, i.e. for each vertex <math display="inline">j</math> in the graph, it can be rewritten as
: <math>\lim_{t\to\infty}\phi_j(t) = \frac{1}{N} \sum_{i = 1}^N c_i(0) </math>.

In other words, at steady state, the value of <math display="inline">\phi</math> converges to the same value at each of the vertices of the graph, which is the average of the initial values at all of the vertices.  Since this is the solution to the heat diffusion equation, this makes perfect sense intuitively.  We expect that neighboring elements in the graph will exchange energy until that energy is spread out evenly throughout all of the elements that are connected to each other.

= Example of the operator on a grid =

File:Graph Laplacian Diffusion Example.gif|thumb|This GIF shows the progression of diffusion, as solved by the graph laplacian technique.  A graph is constructed over a grid, where each pixel in the graph is connected to its 8 bordering pixels.  Values in the image then diffuse smoothly to their neighbors over time via these connections.  This particular image starts off with three strong point values which spill over to their neighbors slowly.  The whole system eventually settles out to the same value at equilibrium.

This section shows an example of a function <math display="inline">\phi</math> diffusing over time through a graph.  The graph in this example is constructed on a 2D discrete grid, with points on the grid connected to their eight neighbors.  Three initial points are specified to have a positive value, while the rest of the values in the grid are zero.  Over time, the exponential decay acts to distribute the values at these points evenly throughout the entire grid.

The complete Matlab source code that was used to generate this animation is provided below.  It shows the process of specifying initial conditions, projecting these initial conditions onto the eigenvalues of the Laplacian Matrix, and simulating the exponential decay of these projected initial conditions.

<source lang=matlab>

N = 20;%The number of pixels along a dimension of the image
A = zeros(N, N);%The image
Adj = zeros(N*N, N*N);%The adjacency matrix

%Use 8 neighbors, and fill in the adjacency matrix
dx = [-1, 0, 1, -1, 1, -1, 0, 1];
dy = [-1, -1, -1, 0, 0, 1, 1, 1];
for x = 1:N
   for y = 1:N
       index = (x-1)*N + y;
       for ne = 1:length(dx)
           newx = x + dx(ne);
           newy = y + dy(ne);
           if newx > 0 && newx <= N && newy > 0 && newy <= N
               index2 = (newx-1)*N + newy;
               Adj(index, index2) = 1;
           end
       end
   end
end

%%%BELOW IS THE KEY CODE THAT COMPUTES THE SOLUTION TO THE DIFFERENTIAL
%%%EQUATION
Deg = diag(sum(Adj, 2));%Compute the degree matrix
L = Deg - Adj;%Compute the laplacian matrix in terms of the degree and adjacency matrices
[V, D] = eig(L);%Compute the eigenvalues/vectors of the laplacian matrix
D = diag(D);

%Initial condition (place a few large positive values around and
%make everything else zero)
C0 = zeros(N, N);
C0(2:5, 2:5) = 5;
C0(10:15, 10:15) = 10;
C0(2:5, 8:13) = 7;
C0 = C0(:);

C0V = V'*C0;%Transform the initial condition into the coordinate system 
%of the eigenvectors
for t = 0:0.05:5
   %Loop through times and decay each initial component
   Phi = C0V.*exp(-D*t);%Exponential decay for each component
   Phi = V*Phi;%Transform from eigenvector coordinate system to original coordinate system
   Phi = reshape(Phi, N, N);
   %Display the results and write to GIF file
   imagesc(Phi);
   caxis([0, 10]);
   title(sprintf('Diffusion t = %3f', t));
   frame = getframe(1);
   im = frame2im(frame);
   [imind, cm] = rgb2ind(im, 256);
   if t  0
      imwrite(imind, cm, 'out.gif', 'gif', 'Loopcount', inf, 'DelayTime', 0.1); 
   else
      imwrite(imind, cm, 'out.gif', 'gif', 'WriteMode', 'append', 'DelayTime', 0.1);
   end
end

</source>

 Approximation to the negative continuous Laplacian 
The graph Laplacian matrix can be further viewed as a matrix form of an approximation to the (positive semi-definite) Laplacian operator obtained by the finite difference method.<ref>citation
 | last1 = Smola | first1 = Alexander J.
 | last2 = Kondor | first2 = Risi
 | contribution = Kernels and regularization on graphs
 | doi = 10.1007/978-3-540-45167-9_12
 | pages = 144–158
 | publisher = Springer
 | series = Lecture Notes in Computer Science
 | title = Learning Theory and Kernel Machines: 16th Annual Conference on Learning Theory and 7th Kernel Workshop, COLT/Kernel 2003, Washington, DC, USA, August 24–27, 2003, Proceedings
 | volume = 2777
 | year = 2003
.</ref> In this interpretation, every graph vertex is treated as a grid point; the local connectivity of the vertex determines the finite difference approximation stencil (numerical analysis)|stencil at this grid point, the grid size is always one for every edge, and there are no constraints on any grid points, which corresponds to the case of the homogeneous Neumann boundary condition, i.e., free boundary.
 
 Directed multigraphs 
An analogue of the Laplacian matrix can be defined for directed multigraphs.<ref name="Chaiken1978">cite journal
 | title = Matrix Tree Theorems 
 | author1=Chaiken, S. | author2=Kleitman, D. | authorlink2=Daniel Kleitman
 | journal = Journal of Combinatorial Theory, Series A 
 | volume = 24
 | number = 3
 | pages = 377–381
 | year = 1978
 | issn = 0097-3165
 | url = http://www.sciencedirect.com/science/article/pii/0097316578900675
 | doi=10.1016/0097-3165(78)90067-5
</ref> In this case the Laplacian matrix ''L'' is defined as
:<math>L = D - A</math>

where ''D'' is a diagonal matrix with ''D''<sub>''i'',''i''</sub> equal to the outdegree of vertex ''i'' and ''A'' is a matrix with ''A''<sub>''i'',''j''</sub> equal to the number of edges from ''i'' to ''j'' (including loops).

 See also 
*Stiffness matrix
*Resistance distance
*Transition rate matrix

 References 

reflist
*T. Sunada, "Discrete geometric analysis", ''Proceedings of Symposia in Pure Mathematics,'' (ed. by P. Exner, J. P. Keating, P. Kuchment, T. Sunada, A. Teplyaev), 77 (2008), 51–86.
*Béla Bollobás|B. Bollobás, ''Modern Graph Theory'', Springer-Verlag (1998, corrected ed. 2013), ISBN|0-387-98488-7, Chapters II.3 (Vector Spaces and Matrices  Associated with Graphs), VIII.2 (The Adjacency Matrix and the Laplacian), IX.2 (Electrical Networks and Random Walks).

Category:Algebraic graph theory
Category:Matrices