{{Distinguish|computational theory of mind}}
{{For|the journal|Theory of Computing}}

[[Image:Maquina.png|thumb|250px|An artistic representation of a [[Turing machine]]. Turing machines are frequently used as theoretical models for computing.]]

In [[theoretical computer science]] and [[mathematics]], the '''theory of computation''' is the branch of  [[theoretical computer science]] that deals with how efficiently problems can be solved on a [[model of computation]], using an [[algorithm]].  The field is divided into three major branches: [[automata theory]] and languages, [[computability theory]], and [[computational complexity theory]], which are linked by the question: ''"What are the fundamental capabilities and limitations of computers?".''<ref name=Sipser-3rd>{{cite book|author = [[Michael Sipser]] | year = 2013 | title = Introduction to the Theory of Computation 3rd |quote=central areas of the theory of computation: automata, computability, and complexity. (Page 1) |publisher =Cengage Learning |isbn=978-1-133-18779-0}}</ref>

In order to perform a rigorous study of computation, computer scientists work with a mathematical abstraction of computers called a [[model of computation]].  There are several models in use, but the most commonly examined is the [[Turing machine]].<ref name=Hodges-2012>{{cite book| first=Andrew | last=Hodges | authorlink=Andrew Hodges | year = 2012 | title =[[Alan Turing: The Enigma]] (The Centenary Edition) | publisher=[[Princeton University Press]] |isbn=978-0-691-15564-7}}</ref> Computer scientists study the Turing machine because it is simple to formulate, can be analyzed and used to prove results, and because it represents what many consider the most powerful possible "reasonable" model of computation (see [[Church–Turing thesis]]).<ref>{{cite video | last = Rabin | first = Michael O. | author-link = Michael O. Rabin | title = Turing, Church, Gödel, Computability, Complexity and Randomization: A Personal View | date = June 2012 | url = http://videolectures.net/turing100_rabin_turing_church_goedel/ }}</ref> It might seem that the potentially infinite memory capacity is an unrealizable attribute, but any [[Decidability (logic)|decidable]] problem<ref name=Monk1976>{{cite book|author =Donald Monk | year = 1976| title =Mathematical Logic| publisher =Springer-Verlag |isbn=9780387901701}}</ref> solved by a Turing machine will always require only a finite amount of memory. So in principle, any problem that can be solved (decided) by a Turing machine can be solved by a computer that has a finite amount of memory.

== History ==
The theory of computation can be considered the creation of models of all kinds in the field of computer science. Therefore, [[Mathematical logic|mathematics and logic]] are used. In the last century it became an independent academic discipline and was separated from mathematics.

Some pioneers of the theory of computation were [[Alonzo Church]], [[Kurt Gödel]], [[Alan Turing]], [[Stephen Kleene]], [[Rózsa Péter]], [[John von Neumann]] and [[Claude Shannon]].

== Branches ==

=== Automata theory ===
{{main|Automata theory}}
{| class="wikitable"
|-
! Grammar
! Languages
! Automaton
! Production rules (constraints)
|-
| Type-0
| [[recursively enumerable language|Recursively enumerable]]
| [[Turing machine]]
| <math>\alpha \rightarrow \beta</math> (no restrictions)
|-
| Type-1
| [[context-sensitive grammar|Context-sensitive]]
| [[Linear bounded automaton|Linear-bounded non-deterministic Turing machine]]
| <math>\alpha A \beta \rightarrow \alpha \gamma \beta</math>
|-
| Type-2
| [[context-free grammar|Context-free]]
| Non-deterministic [[pushdown automaton]]
| <math>A \rightarrow \gamma</math>
|-
| Type-3
| [[regular grammar|Regular]]
| [[Finite state automaton]]
| <math>A \rightarrow a</math><br /> and<br /><math>A \rightarrow aB</math>
|}

Automata theory is the study of abstract machines (or more appropriately, abstract 'mathematical' machines or systems) and the computational problems that can be solved using these machines. These abstract machines are called automata. Automata comes from the Greek word (Αυτόματα) which means that something is doing something by itself.
Automata theory is also closely related to [[formal language]] theory,<ref name=hopcroft-ullman>{{cite book|author =[[John Hopcroft|Hopcroft, John E.]] and [[Jeffrey D. Ullman]]  | year = 2006| title =[[Introduction to Automata Theory, Languages, and Computation]]. 3rd ed| publisher =Reading, MA: Addison-Wesley. |isbn= 978-0-321-45536-9}}</ref> as the automata are often classified by the class of formal languages they are able to recognize. An automaton can be a finite representation of a formal language that may be an infinite set. Automata are used as theoretical models for computing machines, and are used for proofs about computability.

==== Formal Language theory ====
{{main|Formal language}}
[[Image:Chomsky-hierarchy.svg|thumb|right|200px|alt=The Chomsky hierarchy|Set inclusions described by the Chomsky hierarchy]]
Language theory is a branch of mathematics concerned with describing languages as a set of operations over an [[alphabet]]. It is closely linked with automata theory, as automata are used to generate and recognize formal languages. There are several classes of formal languages, each allowing more complex language specification than the one before it, i.e. [[Chomsky hierarchy]],<ref>{{cite journal |last=[[Chomsky hierarchy]] |first= |last2= |first2= |date=1956 |title=Three models for the description of language |url=http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1056813&tag=1 |journal=Information Theory, IRE Transactions on |publisher=IEEE |volume=2 |issue=3 |pages= 113–124|doi=10.1109/TIT.1956.1056813 |accessdate=6 January 2015}}</ref> and each corresponding to a class of automata which recognizes it. Because automata are used as models for computation, formal languages are the preferred mode of specification for any problem that must be computed.

=== Computability theory ===
{{main|Computability theory}}
Computability theory deals primarily with the question of the extent to which a problem is solvable on a computer. The statement that the [[halting problem]] cannot be solved by a Turing machine<ref>{{cite journal |last=[[Alan Turing]] |first= |last2= |first2= |date=1937 |title=On computable numbers, with an application to the Entscheidungsproblem |url=http://www.turingarchive.org/browse.php/B/12 |journal= Proceedings of the [[London Mathematical Society]] |publisher=IEEE |volume=2 |issue=42 |pages=230–265 |doi=10.1112/plms/s2-42.1.230 |accessdate=6 January 2015}}</ref> is one of the most important results in computability theory, as it is an example of a concrete problem that is both easy to formulate and impossible to solve using a Turing machine.  Much of computability theory builds on the halting problem result.

Another important step in computability theory was [[Rice's theorem]], which states that for all non-trivial properties of partial functions, it is [[Decidability (logic)|undecidable]] whether a Turing machine computes a partial function with that property.<ref>{{cite journal |last=Henry Gordon Rice |first= |last2= |first2= |date=1953 |title=Classes of Recursively Enumerable Sets and Their Decision Problems |journal= Transactions of the American Mathematical Society |publisher=American Mathematical Society|volume=74 |issue=2 |pages=358–366 |doi= 10.2307/1990888|jstor=1990888}}</ref>

Computability theory is closely related to the branch of [[mathematical logic]] called [[recursion theory]], which removes the restriction of studying only models of computation which are reducible to the Turing model.<ref name=davis>{{cite book|author =[[Martin Davis]] |year = 2004 |title =The undecidable: Basic papers on undecidable propositions, unsolvable problems and computable functions (Dover Ed) |publisher =Dover Publications |isbn=978-0486432281}}</ref>  Many mathematicians and computational theorists who study recursion theory will refer to it as computability theory.

=== Computational complexity theory ===
{{main|Computational complexity theory}}
[[Image:Complexity subsets pspace.svg|250px|thumb|right|A representation of the relation among complexity classes]]
[[Computational complexity theory|Complexity theory]] considers not only whether a problem can be solved at all on a computer, but also how efficiently the problem can be solved.  Two major aspects are considered: time complexity and space complexity, which are respectively how many steps does it take to perform a computation, and how much memory is required to perform that computation.

In order to analyze how much time and space a given [[algorithm]] requires, computer scientists express the time or space required to solve the problem as a function of the size of the input problem.  For example, finding a particular number in a long list of numbers becomes harder as the list of numbers grows larger.  If we say there are ''n'' numbers in the list, then if the list is not sorted or indexed in any way we may have to look at every number in order to find the number we're seeking.  We thus say that  in order to solve this problem, the computer needs to perform a number of steps that grows linearly in the size of the problem.

To simplify this problem, computer scientists have adopted [[Big O notation]], which allows functions to be compared in a way that ensures that particular aspects of a machine's construction do not need to be considered, but rather only the [[Asymptotic analysis|asymptotic behavior]] as problems become large.  So in our previous example we might say that the problem requires <math>O(n)</math> steps to solve.

Perhaps the most important open problem in all of [[computer science]] is the question of whether a certain broad class of problems denoted [[NP (complexity)|NP]] can be solved efficiently. This is discussed further at [[P = NP problem|Complexity classes P and NP]], and [[P versus NP problem]] is one of the seven [[Millennium Prize Problems]] stated by the [[Clay Mathematics Institute]] in 2000. The [http://www.claymath.org/sites/default/files/pvsnp.pdf Official Problem Description] was given by [[Turing Award]] winner [[Stephen Cook]].

== Models of computation ==<!-- This section is linked from [[Abstract machine]] -->
{{main|Model of computation}}

Aside from a [[Turing machine]], other equivalent (See: [[Church–Turing thesis]]) models of computation are in use.

;[[Lambda calculus]]: A computation consists of an initial lambda expression (or two if you want to separate the function and its input) plus a finite sequence of lambda terms, each deduced from the preceding term by one application of [[Beta reduction]].
;[[Combinatory logic]]
:is a concept which has many similarities to <math>\lambda</math>-calculus, but also important differences exist (e.g. fixed point combinator '''Y''' has normal form in combinatory logic but not in <math>\lambda</math>-calculus). Combinatory logic was developed with great ambitions: understanding the nature of paradoxes, making foundations of mathematics more economic (conceptually), eliminating the notion of variables (thus clarifying their role in mathematics).

;[[μ-recursive function]]s: a computation consists of a mu-recursive function, ''i.e.'' its defining sequence, any input value(s) and a sequence of recursive functions appearing in the defining sequence with inputs and outputs.  Thus, if in the defining sequence of a recursive function <math>f(x)</math> the functions <math>g(x)</math> and <math>h(x,y)</math> appear, then terms of the form 'g(5)=7' or 'h(3,2)=10' might appear.  Each entry in this sequence needs to be an application of a basic function or follow from the entries above by using [[Function composition (computer science)|composition]], [[primitive recursion]] or [[μ-recursive function|μ recursion]].  For instance if <math>f(x)=h(x,g(x))</math>, then for 'f(5)=3' to appear, terms like 'g(5)=6' and 'h(5,6)=3' must occur above.  The computation terminates only if the final term gives the value of the recursive function applied to the inputs.

;[[Markov algorithm]]: a [[string rewriting system]] that uses [[grammar]]-like rules to operate on [[string (computer science)|strings]] of symbols.

;[[Register machine]]
:is a theoretically interesting idealization of a computer. There are several variants. In most of them, each register can hold a natural number (of unlimited size), and the instructions are simple (and few in number), e.g. only decrementation (combined with conditional jump) and incrementation exist (and halting). The lack of the infinite (or dynamically growing) external store (seen at Turing machines) can be understood by replacing its role with [[Gödel numbering]] techniques: the fact that each register holds a natural number allows the possibility of representing a complicated thing (e.g. a sequence, or a matrix etc.) by an appropriate huge natural number — unambiguity of both representation and interpretation can be established by [[number theory|number theoretical]]  foundations of these techniques.

In addition to the general computational models, some simpler computational models are useful for special, restricted applications.  [[Regular expressions]], for example,  specify string patterns in many contexts, from office productivity software to [[programming language]]s. Another formalism mathematically equivalent to regular expressions, [[finite state machines|Finite automata]] are used in circuit design and in some kinds of problem-solving. [[Context-free grammar]]s  specify programming language syntax.  Non-deterministic [[pushdown automaton|pushdown automata]] are another formalism equivalent to context-free grammars. [[Primitive recursive function]]s are a defined subclass of the recursive functions.

Different models of computation have the ability to do different tasks. One way to measure the power of a computational model is to study the class of [[formal language]]s that the model can generate; in such a way to the [[Chomsky hierarchy]] of languages is obtained.

== References ==
{{reflist}}

==Further reading==
;Textbooks aimed at computer scientists
(There are many textbooks in this area; this list is by necessity incomplete.)
* [[John Hopcroft|Hopcroft, John E.]], and [[Jeffrey D. Ullman]] (2006). ''[[Introduction to Automata Theory, Languages, and Computation]]. 3rd ed''  Reading, MA: Addison-Wesley. {{isbn|978-0-321-45536-9}} One of the standard references in the field.
*{{cite book|author=[[Linz P]]|title=An introduction to formal language and automata|publisher=Narosa Publishing|isbn=9788173197819}}
* {{cite book|author = [[Michael Sipser]] | year = 2013 | title = Introduction to the Theory of Computation|edition= 3rd |publisher =Cengage Learning |isbn=978-1-133-18779-0}}
* {{cite book | author = [[Eitan Gurari]] | year = 1989 | title = An Introduction to the Theory of Computation | url = http://www.cse.ohio-state.edu/~gurari/theory-bk/theory-bk.html | publisher = Computer Science Press | isbn = 0-7167-8182-4 | deadurl = yes | archiveurl = https://web.archive.org/web/20070107040625/http://www.cse.ohio-state.edu/~gurari/theory-bk/theory-bk.html | archivedate = 2007-01-07 | df =  }}
* Hein, James L. (1996) ''Theory of Computation.''  Sudbury, MA: Jones & Bartlett.  {{isbn|978-0-86720-497-1}} A gentle introduction to the field, appropriate for second-year undergraduate computer science students.
* Taylor, R. Gregory (1998). ''Models of Computation and Formal Languages.''  New York: Oxford University Press.  {{isbn|978-0-19-510983-2}}  An unusually readable textbook, appropriate for upper-level undergraduates or beginning graduate students.
* Lewis, F. D. (2007). ''[http://cse.ucdenver.edu/~cscialtman/foundation/Essentials%20of%20Theoretical%20Computer%20Science.pdf Essentials of theoretical computer science]'' A textbook covering the topics of formal languages, automata and grammars. The emphasis appears to be on presenting an overview of the results and their applications rather than providing proofs of the results.
* [[Martin Davis]], Ron Sigal, Elaine J. Weyuker, ''Computability, complexity, and languages: fundamentals of theoretical computer science'', 2nd ed., Academic Press, 1994, {{isbn|0-12-206382-1}}. Covers a wider range of topics than most other introductory books, including [[program semantics]] and [[quantification theory]]. Aimed at graduate students.

;Books on computability theory from the (wider) mathematical perspective
* [[Hartley Rogers, Jr]] (1987). ''Theory of Recursive Functions and Effective Computability'', MIT Press. {{isbn|0-262-68052-1}}
* {{cite book|author = [[S. Barry Cooper]] | year = 2004 | title = Computability Theory | publisher = Chapman and Hall/CRC | isbn = 1-58488-237-9}}.
* [[Carl Herbert Smith|Carl H. Smith]], ''A recursive introduction to the theory of computation'', Springer, 1994, {{isbn|0-387-94332-3}}. A shorter textbook suitable for graduate students in Computer Science.

;Historical perspective
* {{cite book|author = [[Richard L. Epstein]] and [[Walter A. Carnielli]] | year = 2000 | title = Computability: Computable Functions, Logic, and the Foundations of Mathematics, with Computability: A Timeline (2nd ed.) | publisher = Wadsworth/Thomson Learning | isbn = 0-534-54644-7}}.

== External links ==
* [http://toc.csail.mit.edu/ Theory of Computation] at [[Massachusetts Institute of Technology|MIT]]
* [http://toc.seas.harvard.edu/ Theory of Computation] at [[Harvard]]
* [http://www.cis.upenn.edu/~giorgi/cl.html Computability Logic] - A theory of interactive computation. The main web source on this subject.

{{Areas of mathematics | state=collapsed}}

[[Category:Theory of computation| ]]