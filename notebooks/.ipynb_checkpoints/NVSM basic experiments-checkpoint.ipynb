{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NVSM(nn.Module):\n",
    "    def __init__(self, n_doc, n_tok, dim_doc_emb, dim_tok_emb, neg_sampling_rate):\n",
    "        super(NVSM, self).__init__()\n",
    "        self.doc_emb           = nn.Embedding(n_doc, embedding_dim = dim_doc_emb)\n",
    "        self.tok_emb           = nn.Embedding(n_tok, embedding_dim = dim_tok_emb)\n",
    "        self.tok_to_doc        = nn.Linear(dim_tok_emb, dim_doc_emb)\n",
    "        self.neg_sampling_rate = neg_sampling_rate\n",
    "        \n",
    "    def query_to_tensor(self, query):\n",
    "        '''\n",
    "        Computes the average of the word embeddings of the query. This method \n",
    "        corresponds to the function 'g' in the article.\n",
    "        '''\n",
    "        # Create a mask to ignore padding embeddings\n",
    "        query_mask    = (query != 0).float()\n",
    "        # Compute the number of tokens in each query to properly compute the \n",
    "        # average\n",
    "        tok_by_input  = query_mask.sum(dim = 1)\n",
    "        query_tok_emb = self.tok_emb(query)\n",
    "        query_tok_emb = query_tok_emb * query_mask.unsqueeze(-1)\n",
    "        # Compute the average of the embeddings\n",
    "        query_emb     = query_tok_emb.sum(dim = 1) / tok_by_input.unsqueeze(-1)\n",
    "        \n",
    "        return query_emb\n",
    "    \n",
    "    def normalize_query_tensor(self, query_tensor):\n",
    "        '''\n",
    "        Divides each query tensor by its L2 norm. This method corresponds to \n",
    "        the function 'norm' in the article.\n",
    "        '''\n",
    "        norm = torch.norm(query_tensor, dim = 1) # we might have to detach this value \n",
    "                                                 # from the computation graph.\n",
    "        return query_tensor / norm.unsqueeze(-1)\n",
    "        \n",
    "    def query_to_doc_space(self, query):\n",
    "        '''\n",
    "        Projects a query vector into the document vector space. This method corresponds \n",
    "        to the function 'f' in the article.\n",
    "        '''\n",
    "        return self.tok_to_doc(query)\n",
    "    \n",
    "    def score(self, query, document):\n",
    "        '''\n",
    "        Computes the cosine similarity between a query and a document embedding.\n",
    "        This method corresponds to the function 'score' in the article.\n",
    "        '''\n",
    "        # batch dot product using batch matrix multiplication\n",
    "        num   = torch.bmm(query.unsqueeze(1), document.unsqueeze(-1))\n",
    "        denum = torch.norm(query, dim = 1) * torch.norm(document, dim = 1)\n",
    "        \n",
    "        return num / denum\n",
    "        \n",
    "    def non_stand_projection(self, n_gram):\n",
    "        '''\n",
    "        Computes the non-standard projection of a n-gram into the document vector \n",
    "        space. This method corresponds to the function 'T^~' in the article.\n",
    "        '''\n",
    "        n_gram_tensor      = self.query_to_tensor(n_gram)\n",
    "        norm_n_gram_tensor = self.normalize_query_tensor(n_gram_tensor)\n",
    "        projection         = self.query_to_doc_space(norm_n_gram_tensor)\n",
    "        \n",
    "        return projection\n",
    "    \n",
    "    def forward(self, query, document_ids):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [\n",
    "        [1, 2, 3, 0, 0], \n",
    "        [4, 5, 6, 7, 8]\n",
    "]\n",
    "t = torch.tensor(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "nvsm = NVSM(\n",
    "    n_doc       = 20, \n",
    "    n_tok       = 9, \n",
    "    dim_doc_emb = 10, \n",
    "    dim_tok_emb = 7,\n",
    "    neg_sampling_rate = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = nvsm.non_stand_projection(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nvsm.query_to_tensor(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (t != 0).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_by_input = mask.sum(dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = nn.Embedding(9, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 10])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_emb = emb(t)\n",
    "t_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1371, -1.0148, -1.0533,  0.4884,  1.2920, -0.4554,  0.2100,\n",
       "           0.3689, -0.8928, -0.9624],\n",
       "         [-0.1229, -1.5732,  0.4160,  1.3271,  0.7516,  0.7500,  1.8796,\n",
       "           1.1547, -0.0250,  0.7064],\n",
       "         [-0.5257, -0.1013, -0.7273, -0.4093, -1.2507, -0.0586, -0.7206,\n",
       "          -1.8130,  0.7023, -0.1104],\n",
       "         [-0.4782, -0.1574,  0.6121,  0.0597,  0.1085,  0.1639,  1.5457,\n",
       "           0.2678, -0.9187,  0.9393],\n",
       "         [-0.4782, -0.1574,  0.6121,  0.0597,  0.1085,  0.1639,  1.5457,\n",
       "           0.2678, -0.9187,  0.9393]],\n",
       "\n",
       "        [[ 0.9484, -0.8201, -1.2047, -1.3277,  1.9986,  0.4751, -1.0518,\n",
       "          -0.0499, -1.2592,  0.1978],\n",
       "         [-0.5886, -0.1557, -1.6149, -1.3309, -0.9811,  2.5172,  2.1142,\n",
       "           0.4247,  0.3768,  0.1469],\n",
       "         [-1.9336,  1.9258,  0.3230,  0.1779,  0.6453,  2.0279,  0.3779,\n",
       "          -0.2205, -0.7898, -1.2786],\n",
       "         [ 1.6658,  0.7742,  1.5495,  0.4461, -1.0913,  0.7044, -0.9092,\n",
       "          -0.8003, -0.2587,  0.4214],\n",
       "         [-0.1851, -0.2427, -0.4881,  0.1121, -0.0495, -1.7932,  0.8425,\n",
       "          -0.7243,  1.4420, -0.6067]]], grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1371, -1.0148, -1.0533,  0.4884,  1.2920, -0.4554,  0.2100,\n",
       "           0.3689, -0.8928, -0.9624],\n",
       "         [-0.1229, -1.5732,  0.4160,  1.3271,  0.7516,  0.7500,  1.8796,\n",
       "           1.1547, -0.0250,  0.7064],\n",
       "         [-0.5257, -0.1013, -0.7273, -0.4093, -1.2507, -0.0586, -0.7206,\n",
       "          -1.8130,  0.7023, -0.1104],\n",
       "         [-0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000, -0.0000,  0.0000],\n",
       "         [-0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000, -0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.9484, -0.8201, -1.2047, -1.3277,  1.9986,  0.4751, -1.0518,\n",
       "          -0.0499, -1.2592,  0.1978],\n",
       "         [-0.5886, -0.1557, -1.6149, -1.3309, -0.9811,  2.5172,  2.1142,\n",
       "           0.4247,  0.3768,  0.1469],\n",
       "         [-1.9336,  1.9258,  0.3230,  0.1779,  0.6453,  2.0279,  0.3779,\n",
       "          -0.2205, -0.7898, -1.2786],\n",
       "         [ 1.6658,  0.7742,  1.5495,  0.4461, -1.0913,  0.7044, -0.9092,\n",
       "          -0.8003, -0.2587,  0.4214],\n",
       "         [-0.1851, -0.2427, -0.4881,  0.1121, -0.0495, -1.7932,  0.8425,\n",
       "          -0.7243,  1.4420, -0.6067]]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_emb * mask.unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_embs = t_emb * mask.unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratata = tok_embs.sum(dim = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_by_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7856, -2.6893, -1.3645,  1.4062,  0.7928,  0.2359,  1.3690, -0.2894,\n",
       "         -0.2155, -0.3665],\n",
       "        [-0.0931,  1.4814, -1.4352, -1.9225,  0.5221,  3.9315,  1.3737, -1.3703,\n",
       "         -0.4889, -1.1191]], grad_fn=<SumBackward2>)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratata "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2619, -0.8964, -0.4548,  0.4687,  0.2643,  0.0786,  0.4563, -0.0965,\n",
       "         -0.0718, -0.1222],\n",
       "        [-0.0186,  0.2963, -0.2870, -0.3845,  0.1044,  0.7863,  0.2747, -0.2741,\n",
       "         -0.0978, -0.2238]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratata / t_by_input.unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 5.])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_by_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tens = torch.tensor([\n",
    "    [1., 1.],\n",
    "    [1., 0.],\n",
    "    [0., 1.]\n",
    "])\n",
    "tens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.4142, 1.0000, 1.0000])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tens_norm = torch.norm(tens, dim = 1)\n",
    "tens_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-185d03fb8bd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtens\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtens_norm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "tens / tens_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.4142, 1.0000, 1.0000])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tens_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (2) must match the existing size (3) at non-singleton dimension 1.  Target sizes: [3, 2].  Tensor sizes: [3]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-115-d0ee1eb7a4c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtens_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (2) must match the existing size (3) at non-singleton dimension 1.  Target sizes: [3, 2].  Tensor sizes: [3]"
     ]
    }
   ],
   "source": [
    "tens_norm.expand_as(tens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "tens_norm.expand_as?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "tens_norm.expand?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "tens.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7071, 0.7071],\n",
       "        [1.0000, 0.0000],\n",
       "        [0.0000, 1.0000]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tens / torch.norm(tens, dim = 1).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.dot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.bmm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 2.],\n",
       "         [3., 4.]]), tensor([[2., 0.],\n",
       "         [1., 3.]]))"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = torch.tensor([\n",
    "    [1., 2.],\n",
    "    [3., 4.]\n",
    "])\n",
    "\n",
    "t2 = torch.tensor([\n",
    "    [2., 0.],\n",
    "    [1., 3.]\n",
    "])\n",
    "\n",
    "t1, t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = torch.bmm(t1.unsqueeze(1), t2.unsqueeze(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4472, 0.9487])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.squeeze() / (torch.norm(t1, dim = 1) * torch.norm(t2, dim = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.2361, 5.0000])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(t1, dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
