{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NVSM(nn.Module):\n",
    "    def __init__(self, n_doc, n_tok, dim_doc_emb, dim_tok_emb, neg_sampling_rate):\n",
    "        super(NVSM, self).__init__()\n",
    "        self.doc_emb           = nn.Embedding(n_doc, embedding_dim = dim_doc_emb)\n",
    "        self.tok_emb           = nn.Embedding(n_tok, embedding_dim = dim_tok_emb)\n",
    "        self.tok_to_doc        = nn.Linear(dim_doc_emb, dim_tok_emb)\n",
    "        self.neg_sampling_rate = neg_sampling_rate\n",
    "        \n",
    "    def query_to_tensor(self, query):\n",
    "        '''\n",
    "        Computes the average of the word embeddings of the query. This method \n",
    "        corresponds to the function 'g' in the article.\n",
    "        '''\n",
    "        # Create a mask to ignore padding embeddings\n",
    "        query_mask    = (query != 0).float()\n",
    "        # Compute the number of tokens in each query to properly compute the \n",
    "        # average\n",
    "        tok_by_input  = query_mask.sum(dim = 1)\n",
    "        query_tok_emb = self.tok_emb(query)\n",
    "        query_tok_emb = query_tok_emb * query_mask.unsqueeze(-1)\n",
    "        # Compute the average of the embeddings\n",
    "        query_emb     = query_tok_emb.sum(dim = 1) / tok_by_input.unsqueeze(-1)\n",
    "        \n",
    "        return query_emb\n",
    "    \n",
    "    def normalize_query_tensor(self, query_tensor):\n",
    "        '''\n",
    "        Divide each query tensor by its L2 norm. This method corresponds to \n",
    "        the function 'norm' in the article.\n",
    "        '''\n",
    "        norm = torch.norm(query_tensor, dim = 1) # we might have to detach this value \n",
    "                                                 # from the computation graph.\n",
    "        return query_tensor / norm.unsqueeze(-1)\n",
    "        \n",
    "    def query_to_doc_space(self, query):\n",
    "        '''\n",
    "        Project a query vector into the document vector space. This method corresponds \n",
    "        to the function 'f' in the paper.\n",
    "        '''\n",
    "        return self.linear(query)\n",
    "        \n",
    "    def forward(self, query, document_ids):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [\n",
    "        [1, 2, 3, 0, 0], \n",
    "        [4, 5, 6, 7, 8]\n",
    "]\n",
    "t = torch.tensor(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "nvsm = NVSM(\n",
    "    n_doc       = 20, \n",
    "    n_tok       = 9, \n",
    "    dim_doc_emb = 10, \n",
    "    dim_tok_emb = 7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_mask tensor([[1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1.]])\n",
      "tok_by_input tensor([3., 5.])\n",
      "query_tok_emb.shape torch.Size([2, 5, 7])\n",
      "raw query_tok_emb tensor([[[ 6.8339e-01,  1.1856e-01,  5.2280e-01, -1.3694e+00, -1.7462e-01,\n",
      "           5.6550e-01, -3.3361e-01],\n",
      "         [-8.7057e-01,  1.4503e-01, -1.1387e+00,  1.2351e-01, -1.1690e+00,\n",
      "          -1.1901e+00, -4.3799e-01],\n",
      "         [ 2.5456e-01, -1.5504e-02,  2.4088e-01,  4.0717e-01, -2.7572e-02,\n",
      "           3.8714e-02,  1.5409e-01],\n",
      "         [-1.5461e-02, -6.2406e-02,  1.9464e+00,  2.8442e-01,  5.5147e-01,\n",
      "          -7.0005e-01,  1.8206e-02],\n",
      "         [-1.5461e-02, -6.2406e-02,  1.9464e+00,  2.8442e-01,  5.5147e-01,\n",
      "          -7.0005e-01,  1.8206e-02]],\n",
      "\n",
      "        [[-2.1760e-01,  7.9879e-02,  7.1749e-01,  5.0667e-01,  7.5838e-01,\n",
      "          -7.7784e-02, -1.3649e+00],\n",
      "         [ 8.4118e-01, -2.6891e-02,  3.9473e-01,  1.9589e-01, -2.1137e-03,\n",
      "          -1.0450e+00, -1.5264e+00],\n",
      "         [-4.0436e-01, -3.6638e-01,  1.0578e+00, -1.2200e+00, -7.3964e-01,\n",
      "          -6.4697e-01,  4.8916e-01],\n",
      "         [-3.0162e-01, -2.0889e+00, -1.3564e+00, -2.2226e-01, -2.7925e-01,\n",
      "          -4.4511e-01,  2.8414e+00],\n",
      "         [ 5.5718e-01, -2.2741e-01,  5.7770e-01, -1.5068e-01, -7.9952e-02,\n",
      "          -9.7733e-01, -1.6848e+00]]], grad_fn=<EmbeddingBackward>)\n",
      "masked query_tok_emb tensor([[[ 6.8339e-01,  1.1856e-01,  5.2280e-01, -1.3694e+00, -1.7462e-01,\n",
      "           5.6550e-01, -3.3361e-01],\n",
      "         [-8.7057e-01,  1.4503e-01, -1.1387e+00,  1.2351e-01, -1.1690e+00,\n",
      "          -1.1901e+00, -4.3799e-01],\n",
      "         [ 2.5456e-01, -1.5504e-02,  2.4088e-01,  4.0717e-01, -2.7572e-02,\n",
      "           3.8714e-02,  1.5409e-01],\n",
      "         [-0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          -0.0000e+00,  0.0000e+00],\n",
      "         [-0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          -0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[-2.1760e-01,  7.9879e-02,  7.1749e-01,  5.0667e-01,  7.5838e-01,\n",
      "          -7.7784e-02, -1.3649e+00],\n",
      "         [ 8.4118e-01, -2.6891e-02,  3.9473e-01,  1.9589e-01, -2.1137e-03,\n",
      "          -1.0450e+00, -1.5264e+00],\n",
      "         [-4.0436e-01, -3.6638e-01,  1.0578e+00, -1.2200e+00, -7.3964e-01,\n",
      "          -6.4697e-01,  4.8916e-01],\n",
      "         [-3.0162e-01, -2.0889e+00, -1.3564e+00, -2.2226e-01, -2.7925e-01,\n",
      "          -4.4511e-01,  2.8414e+00],\n",
      "         [ 5.5718e-01, -2.2741e-01,  5.7770e-01, -1.5068e-01, -7.9952e-02,\n",
      "          -9.7733e-01, -1.6848e+00]]], grad_fn=<MulBackward0>)\n",
      "final result tensor([[ 0.0225,  0.0827, -0.1250, -0.2796, -0.4571, -0.1953, -0.2058],\n",
      "        [ 0.0950, -0.5259,  0.2783, -0.1781, -0.0685, -0.6384, -0.2491]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0225,  0.0827, -0.1250, -0.2796, -0.4571, -0.1953, -0.2058],\n",
       "        [ 0.0950, -0.5259,  0.2783, -0.1781, -0.0685, -0.6384, -0.2491]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nvsm.query_to_tensor(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (t != 0).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_by_input = mask.sum(dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = nn.Embedding(9, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 10])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_emb = emb(t)\n",
    "t_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1371, -1.0148, -1.0533,  0.4884,  1.2920, -0.4554,  0.2100,\n",
       "           0.3689, -0.8928, -0.9624],\n",
       "         [-0.1229, -1.5732,  0.4160,  1.3271,  0.7516,  0.7500,  1.8796,\n",
       "           1.1547, -0.0250,  0.7064],\n",
       "         [-0.5257, -0.1013, -0.7273, -0.4093, -1.2507, -0.0586, -0.7206,\n",
       "          -1.8130,  0.7023, -0.1104],\n",
       "         [-0.4782, -0.1574,  0.6121,  0.0597,  0.1085,  0.1639,  1.5457,\n",
       "           0.2678, -0.9187,  0.9393],\n",
       "         [-0.4782, -0.1574,  0.6121,  0.0597,  0.1085,  0.1639,  1.5457,\n",
       "           0.2678, -0.9187,  0.9393]],\n",
       "\n",
       "        [[ 0.9484, -0.8201, -1.2047, -1.3277,  1.9986,  0.4751, -1.0518,\n",
       "          -0.0499, -1.2592,  0.1978],\n",
       "         [-0.5886, -0.1557, -1.6149, -1.3309, -0.9811,  2.5172,  2.1142,\n",
       "           0.4247,  0.3768,  0.1469],\n",
       "         [-1.9336,  1.9258,  0.3230,  0.1779,  0.6453,  2.0279,  0.3779,\n",
       "          -0.2205, -0.7898, -1.2786],\n",
       "         [ 1.6658,  0.7742,  1.5495,  0.4461, -1.0913,  0.7044, -0.9092,\n",
       "          -0.8003, -0.2587,  0.4214],\n",
       "         [-0.1851, -0.2427, -0.4881,  0.1121, -0.0495, -1.7932,  0.8425,\n",
       "          -0.7243,  1.4420, -0.6067]]], grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1371, -1.0148, -1.0533,  0.4884,  1.2920, -0.4554,  0.2100,\n",
       "           0.3689, -0.8928, -0.9624],\n",
       "         [-0.1229, -1.5732,  0.4160,  1.3271,  0.7516,  0.7500,  1.8796,\n",
       "           1.1547, -0.0250,  0.7064],\n",
       "         [-0.5257, -0.1013, -0.7273, -0.4093, -1.2507, -0.0586, -0.7206,\n",
       "          -1.8130,  0.7023, -0.1104],\n",
       "         [-0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000, -0.0000,  0.0000],\n",
       "         [-0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000, -0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.9484, -0.8201, -1.2047, -1.3277,  1.9986,  0.4751, -1.0518,\n",
       "          -0.0499, -1.2592,  0.1978],\n",
       "         [-0.5886, -0.1557, -1.6149, -1.3309, -0.9811,  2.5172,  2.1142,\n",
       "           0.4247,  0.3768,  0.1469],\n",
       "         [-1.9336,  1.9258,  0.3230,  0.1779,  0.6453,  2.0279,  0.3779,\n",
       "          -0.2205, -0.7898, -1.2786],\n",
       "         [ 1.6658,  0.7742,  1.5495,  0.4461, -1.0913,  0.7044, -0.9092,\n",
       "          -0.8003, -0.2587,  0.4214],\n",
       "         [-0.1851, -0.2427, -0.4881,  0.1121, -0.0495, -1.7932,  0.8425,\n",
       "          -0.7243,  1.4420, -0.6067]]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_emb * mask.unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_embs = t_emb * mask.unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratata = tok_embs.sum(dim = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_by_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7856, -2.6893, -1.3645,  1.4062,  0.7928,  0.2359,  1.3690, -0.2894,\n",
       "         -0.2155, -0.3665],\n",
       "        [-0.0931,  1.4814, -1.4352, -1.9225,  0.5221,  3.9315,  1.3737, -1.3703,\n",
       "         -0.4889, -1.1191]], grad_fn=<SumBackward2>)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratata "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2619, -0.8964, -0.4548,  0.4687,  0.2643,  0.0786,  0.4563, -0.0965,\n",
       "         -0.0718, -0.1222],\n",
       "        [-0.0186,  0.2963, -0.2870, -0.3845,  0.1044,  0.7863,  0.2747, -0.2741,\n",
       "         -0.0978, -0.2238]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratata / t_by_input.unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 5.])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_by_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tens = torch.tensor([\n",
    "    [1., 1.],\n",
    "    [1., 0.],\n",
    "    [0., 1.]\n",
    "])\n",
    "tens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.4142, 1.0000, 1.0000])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tens_norm = torch.norm(tens, dim = 1)\n",
    "tens_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-185d03fb8bd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtens\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtens_norm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "tens / tens_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.4142, 1.0000, 1.0000])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tens_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (2) must match the existing size (3) at non-singleton dimension 1.  Target sizes: [3, 2].  Tensor sizes: [3]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-115-d0ee1eb7a4c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtens_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (2) must match the existing size (3) at non-singleton dimension 1.  Target sizes: [3, 2].  Tensor sizes: [3]"
     ]
    }
   ],
   "source": [
    "tens_norm.expand_as(tens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "tens_norm.expand_as?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "tens_norm.expand?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "tens.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7071, 0.7071],\n",
       "        [1.0000, 0.0000],\n",
       "        [0.0000, 1.0000]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tens / torch.norm(tens, dim = 1).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
