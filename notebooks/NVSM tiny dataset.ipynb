{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NVSM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NVSM(nn.Module):\n",
    "    def __init__(self, n_doc, n_tok, dim_doc_emb, dim_tok_emb, neg_sampling_rate, \n",
    "                 pad_token_id):\n",
    "        super(NVSM, self).__init__()\n",
    "        self.doc_emb           = nn.Embedding(n_doc, embedding_dim = dim_doc_emb)\n",
    "        self.tok_emb           = nn.Embedding(n_tok, embedding_dim = dim_tok_emb)\n",
    "        self.tok_to_doc        = nn.Linear(dim_tok_emb, dim_doc_emb)\n",
    "        self.bias              = nn.Parameter(torch.Tensor(dim_doc_emb))\n",
    "        self.neg_sampling_rate = neg_sampling_rate\n",
    "        self.pad_token_id      = pad_token_id\n",
    "        \n",
    "    def query_to_tensor(self, query):\n",
    "        '''\n",
    "        Computes the average of the word embeddings of the query. This method \n",
    "        corresponds to the function 'g' in the article.\n",
    "        '''\n",
    "        # Create a mask to ignore padding embeddings\n",
    "        query_mask    = (query != self.pad_token_id).float()\n",
    "        # Compute the number of tokens in each query to properly compute the \n",
    "        # average\n",
    "        tok_by_input  = query_mask.sum(dim = 1)\n",
    "        query_tok_emb = self.tok_emb(query)\n",
    "        query_tok_emb = query_tok_emb * query_mask.unsqueeze(-1)\n",
    "        # Compute the average of the embeddings\n",
    "        query_emb     = query_tok_emb.sum(dim = 1) / tok_by_input.unsqueeze(-1)\n",
    "        \n",
    "        return query_emb\n",
    "    \n",
    "    def normalize_query_tensor(self, query_tensor):\n",
    "        '''\n",
    "        Divides each query tensor by its L2 norm. This method corresponds to \n",
    "        the function 'norm' in the article.\n",
    "        '''\n",
    "        norm = torch.norm(query_tensor, dim = 1) # we might have to detach this value \n",
    "                                                 # from the computation graph.\n",
    "        return query_tensor / norm.unsqueeze(-1)\n",
    "        \n",
    "    def query_to_doc_space(self, query):\n",
    "        '''\n",
    "        Projects a query vector into the document vector space. This method corresponds \n",
    "        to the function 'f' in the article.\n",
    "        '''\n",
    "        return self.tok_to_doc(query)\n",
    "    \n",
    "    def score(self, query, document):\n",
    "        '''\n",
    "        Computes the cosine similarity between a query and a document embedding.\n",
    "        This method corresponds to the function 'score' in the article.\n",
    "        '''\n",
    "        # batch dot product using batch matrix multiplication\n",
    "        num   = torch.bmm(query.unsqueeze(1), document.unsqueeze(-1))\n",
    "        denum = torch.norm(query, dim = 1) * torch.norm(document, dim = 1)\n",
    "        \n",
    "        return num / denum\n",
    "        \n",
    "    def non_stand_projection(self, n_gram):\n",
    "        '''\n",
    "        Computes the non-standard projection of a n-gram into the document vector \n",
    "        space. This method corresponds to the function 'T^~' in the article.\n",
    "        '''\n",
    "        n_gram_tensor      = self.query_to_tensor(n_gram)\n",
    "        norm_n_gram_tensor = self.normalize_query_tensor(n_gram_tensor)\n",
    "        projection         = self.query_to_doc_space(norm_n_gram_tensor)\n",
    "        \n",
    "        return projection\n",
    "    \n",
    "    def _custom_batchnorm(self, batch):\n",
    "        '''\n",
    "        Computes the variant of the batch normalization formula used in this article. \n",
    "        It only uses a bias and no weights.\n",
    "        '''\n",
    "        batch_feat_norm = (batch - batch.mean(dim = 0)) / batch.std(dim = 0)\n",
    "        batch_feat_norm = batch_feat_norm + self.bias\n",
    "        \n",
    "        return batch_feat_norm\n",
    "    \n",
    "    def stand_projection(self, batch):\n",
    "        '''\n",
    "        Computes the standard projection of a n-gram into document vector space with\n",
    "        a hardtanh activation. This method corresponds to the function 'T' in the \n",
    "        article.\n",
    "        '''\n",
    "        non_stand_proj = self.non_stand_projection(batch) \n",
    "        bn             = self._custom_batchnorm(non_stand_proj)\n",
    "        activation     = F.hardtanh(bn)\n",
    "\n",
    "        return activation\n",
    "    \n",
    "    def representation_similarity(self, query, document):\n",
    "        '''\n",
    "        Computes the similarity between a query and a document. This method corresponds \n",
    "        to the function 'P' in the article.\n",
    "        '''\n",
    "        document_emb  = self.doc_emb(document)\n",
    "        query_proj    = self.stand_projection(query)\n",
    "        # If we have a single document to match against each query, we have\n",
    "        # to reshape the tensor to compute a simple dot product.\n",
    "        # Otherwise, we compute a simple matrix multiplication to match the \n",
    "        # query against each document.\n",
    "        if len(document_emb.shape) == 2:\n",
    "            document_emb = document_emb.unsqueeze(1)\n",
    "        dot_product   = torch.bmm(document_emb, query_proj.unsqueeze(-1))\n",
    "        similarity    = torch.sigmoid(dot_product)\n",
    "        \n",
    "        return similarity.squeeze()\n",
    "    \n",
    "    def forward(self, query, document):\n",
    "        '''\n",
    "        Approximates the probability of document given query by uniformly sampling \n",
    "        constrastive examples. This method corresponds to the 'P^~' function in the \n",
    "        article.\n",
    "        '''\n",
    "        # Positive term, this should be maximized as it indicates how similar the\n",
    "        # correct document is to the query\n",
    "        pos_repr = self.representation_similarity(query, document)\n",
    "        \n",
    "        # Sampling uniformly 'self.neg_sampling_rate' documents to compute the \n",
    "        # negative term. We first randomly draw the indices of the documents and \n",
    "        # then we compute the similarity with the query.\n",
    "        z               = self.neg_sampling_rate # corresponds to the z variable in \n",
    "                                                 # the article\n",
    "        n_docs          = self.doc_emb.num_embeddings\n",
    "        neg_sample_size = (query.size(0), z)\n",
    "        neg_sample      = torch.randint(low = 0, high = n_docs, size = neg_sample_size)\n",
    "        neg_repr        = self.representation_similarity(query, neg_sample)\n",
    "        \n",
    "        # Probability computation\n",
    "        positive_term = torch.log(pos_repr)\n",
    "        negative_term = torch.log(1 - neg_repr).sum(dim = 1)\n",
    "        proba         = ((z + 1) / (2 * z)) * (z * positive_term + negative_term)\n",
    "        \n",
    "        return proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(nvsm, pred, lamb):\n",
    "    output_term = pred.mean()\n",
    "    sum_square  = lambda m: (m.weight * m.weight).sum()\n",
    "    reg_term    = sum_square(nvsm.tok_emb) + \\\n",
    "                  sum_square(nvsm.doc_emb) + \\\n",
    "                  sum_square(nvsm.tok_to_doc)\n",
    "    loss        = -output_term + (lamb / (2 * pred.shape[0])) * reg_term\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_en = spacy.load('en')\n",
    "\n",
    "def tokenize(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_docs(filepaths):\n",
    "    documents = []\n",
    "    for filepath in filepaths:\n",
    "        with open(filepath) as file:\n",
    "            documents.append(file.read().strip().lower())\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_docs(documents):\n",
    "    tokenized_documents = [tokenize(doc) for doc in documents]\n",
    "#     print([len(doc_tok) for doc_tok in tokenized_documents])\n",
    "    \n",
    "    return tokenized_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary(tokenized_documents):\n",
    "    vocabulary    = {token for doc in tokenized_documents for token in doc}\n",
    "    stoi          = {token : i + 2 for i, token in enumerate(vocabulary)}\n",
    "    stoi['<PAD>'] = 0\n",
    "    stoi['<UNK>'] = 1\n",
    "    itos          = {i : token for token, i in stoi.items()}\n",
    "    \n",
    "    return vocabulary, stoi, itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(tok_docs, stoi, n):\n",
    "    n_grams      = []\n",
    "    document_ids = []\n",
    "    for i, doc in enumerate(tok_docs):\n",
    "        doc_tok_ids = [stoi[tok] for tok in doc]\n",
    "        for n_gram in [doc_tok_ids[i : i + n] for i in range(len(doc) - n)]:\n",
    "            n_grams.append(n_gram)\n",
    "            document_ids.append(i)\n",
    "            \n",
    "    return n_grams, document_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pytorch_datasets(n_grams, doc_ids, val_prop = 0.2):\n",
    "    n_grams_tensor = torch.tensor(n_grams)\n",
    "    doc_ids_tensor = torch.tensor(doc_ids)\n",
    "    full_dataset   = TensorDataset(n_grams_tensor, doc_ids_tensor)\n",
    "    total_size     = len(full_dataset)\n",
    "    val_size       = round(total_size * val_prop)\n",
    "    train, val     = random_split(full_dataset, [total_size - val_size, val_size])\n",
    "    \n",
    "    return train, val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(nvsm, device, optimizer, epochs, train_loader, lamb, print_every):\n",
    "    for epoch in range(epochs):\n",
    "        for i, (n_grams, doc_ids) in enumerate(train_loader):\n",
    "            n_grams    = n_grams.to(device)\n",
    "            doc_ids    = doc_ids.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            pred_proba = nvsm(n_grams, doc_ids)\n",
    "            loss       = loss_function(nvsm, pred_proba, lamb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if i % print_every == 0:\n",
    "                print(f'[{epoch},{i}]: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    filepaths = [\n",
    "        '../data/raw/language/Word_formation',\n",
    "        '../data/raw/language/Terminology',    \n",
    "        '../data/raw/history/Jacobin',\n",
    "        '../data/raw/history/French_Revolution',\n",
    "        '../data/raw/math/Game_theory',\n",
    "        '../data/raw/math/Laplacian_matrix'\n",
    "    ]\n",
    "    documents             = load_docs(filepaths)\n",
    "    tokenized_documents   = tokenize_docs(documents)\n",
    "    voc, stoi, itos       = create_vocabulary(tokenized_documents)\n",
    "    n_grams, document_ids = create_dataset(tokenized_documents, stoi, 10)\n",
    "    train_data, val_data  = create_pytorch_datasets(n_grams, document_ids)\n",
    "    train_loader          = DataLoader(train_data, batch_size = 5000, shuffle = True)\n",
    "    device                = torch.device('cuda')\n",
    "    lamb                  = 1e-3 # regularization weight in the loss\n",
    "    nvsm                  = NVSM(\n",
    "        n_doc             = len(tokenized_documents), \n",
    "        n_tok             = len(stoi), \n",
    "        dim_doc_emb       = 20, \n",
    "        dim_tok_emb       = 30,\n",
    "        neg_sampling_rate = 4,\n",
    "        pad_token_id      = 0\n",
    "    ).to(device)\n",
    "    optimizer             = optim.Adam(nvsm.parameters(), lr = 1e-3)\n",
    "    train(nvsm, device, optimizer, 10, train_loader, lamb, 2)\n",
    "    \n",
    "    return nvsm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of backend CUDA but got backend CPU for argument #3 'index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-80ce61eefcd0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnvsm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-ffa1671df3c0>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m     ).to(device)\n\u001b[1;32m     26\u001b[0m     \u001b[0moptimizer\u001b[0m             \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnvsm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnvsm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlamb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnvsm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-a02a7f6921a9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(nvsm, device, optimizer, epochs, train_loader, lamb, print_every)\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mdoc_ids\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mdoc_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m             \u001b[0mpred_proba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnvsm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_grams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m             \u001b[0mloss\u001b[0m       \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnvsm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_proba\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlamb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nvsm_pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-3f3d57ca5125>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, document)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mneg_sample_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mneg_sample\u001b[0m      \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhigh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_docs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneg_sample_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0mneg_repr\u001b[0m        \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepresentation_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;31m# Probability computation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-3f3d57ca5125>\u001b[0m in \u001b[0;36mrepresentation_similarity\u001b[0;34m(self, query, document)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0;34m'P'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0marticle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         '''\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0mdocument_emb\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0mquery_proj\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstand_projection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;31m# If we have a single document to match against each query, we have\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nvsm_pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nvsm_pytorch/lib/python3.7/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m         return F.embedding(\n\u001b[1;32m    117\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nvsm_pytorch/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1452\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1454\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of backend CUDA but got backend CPU for argument #3 'index'"
     ]
    }
   ],
   "source": [
    "nvsm = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader = DataLoader(train_data, batch_size = 5000, shuffle = True)\n",
    "\n",
    "# nvsm = NVSM(\n",
    "#     n_doc             = 6, \n",
    "#     n_tok             = 10811, \n",
    "#     dim_doc_emb       = 20, \n",
    "#     dim_tok_emb       = 30,\n",
    "#     neg_sampling_rate = 4,\n",
    "#     pad_token_id      = 0\n",
    "# )\n",
    "\n",
    "# for epoch in range(10):\n",
    "#     for i, (n_grams, doc_ids) in enumerate(train_loader):\n",
    "#         optimizer.zero_grad()\n",
    "#         pred_proba = nvsm(n_grams, doc_ids)\n",
    "#         loss = loss_function(nvsm, pred_proba, lamb)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         if i % 5 == 0:\n",
    "#             print(f'[{epoch},{i}]: {loss}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
